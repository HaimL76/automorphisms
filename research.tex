\documentclass[12pt]{article}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{amsfonts, amssymb}
\usepackage{mathrsfs, mathdots} 
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{setspace}
\usepackage{xfrac}
\usepackage{yfonts}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }
 {
  (
  \alec_cycle:nn { #1 } { #2 }
  )
 }

\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2
 {
  \seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }
  \seq_use:Nn \l_alec_cycle_seq { #1 }
 }
\ExplSyntaxOff

\begin{document}

\textbf{Annotations}\newline
\textbf{$F_{ij}(\alpha)$}, where $i<j$, is a $n \times n$ matrix, satisfying, 
$$
F_{ij}(\alpha)=(a_{kl})=\begin{cases}
			\alpha, & k=i \land l=j\\
            0, & \text{otherwise}
		 \end{cases}
$$
Define $F_{ij}:=F{ij}(1)$ \newline
\textbf{$E_{ij}(\alpha)$}, where $i<j$, is a $n \times n$ matrix, satisfying, 
$$
E_{ij}(\alpha)=(a_{kl})=\begin{cases}
            1, & k=l\\
			\alpha, & k=i \land l=j\\
            0, & \text{otherwise}
		 \end{cases}
$$
Define $E_{ij}:=E{ij}(1)$ \newline

\section{The group $U_n$}
%\textbf{1. The group $U_n$} 

\begin{proposition} \label{prop:elem.oper.row.add}
Let $A$ be any $n \times n$ matrix. Then multiplying $A$ from the left by any $E_{ij}(\alpha)$, of the same dimensions, yields a result matrix, $B=E_{ij}(\alpha)A$, whose rows are $$
B_k=\begin{cases}
		A_i+\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
In words, all the rows of $B$ are the rows of $A$, except for row $i$ of $B$, which is the addition of row $i$ of $A$ and the multiplication of row $j$ of A by the scalar $\alpha$.
\end{proposition}
\begin{proof}
Let the elements of $A$ be $(a_{kl})$, and the elements of $E_{ij}(\alpha)$ be $(e_{kl})$. Set the result matrix $B=E_{ij}(\alpha)A$, and let its elements be $(b_{kl})$. For each cell $b_{kl}=\sum_{r=1}^n e_{kr}a_{rl}$. For $k=i$, the sum, for each column $l$, is $0+\dots+0+e{ii}a_{il}+0+\dots+0+e_{ij}a_{jl}+0\dots+0=1a_{il}+\alpha a_{jl}=a_{il}+\alpha a_{jl}$, which proves that $B_i=A_i+\alpha A_j$. The rest is obvious as well.
\end{proof}
\begin{corollary}
\label{cor:inv.elem.oper.row.add}
Let $E_{ij}(\alpha)$ be a matrix of the described form. Then $E_{ij}(\alpha)$ has an inverse matrix, and its inverse is $E_{ij}(\alpha)^{-1}=E_{ij}(-\alpha)$
\end{corollary}
\begin{proof}
Easy to observe that $|E_{ij}(\alpha)|=1\neq 0$, so there exists a matrix $B=E_{ij}(\alpha)^{-1}$. Let the elements of $E_{ij}(\alpha)$ be $(e_{kl})$, and let the elements of $B$ be $(b_{kl})$. From proposition~\ref{prop:elem.oper.row.add}, we know that multiplying $B$ from the left by $E_{ij}(\alpha)$ yields a matrix $C$ with all the rows identical to the rows of $B$, except for row $i$, which is the addition $B_i+\alpha B_j$. Let the elements of $C$ be $(c_{kl})$. But $B=E_{ij}(\alpha)^{-1}$, which means $C=I_n$, so 
$$
c_{kl}=\begin{cases}
		1, & k=l\\
            0, & \text{otherwise}
		 \end{cases}
$$
This yields the following equations,
$$
\begin{cases}
c_{ii}=1=b_{ii}+\alpha b_{ji}\\
c_{ij}=0=b_{ij}+\alpha b_{jj}\\
		 \end{cases}
$$
But, $C_j=B_j$, which means that $c_{jj}=1=b_{jj}$. Taking this to the second equation, gives $0=b_{ij}+\alpha 1 \Rightarrow b_{ij}=\alpha$. Similarly, $c_{ji}=0=b_{ji}$. Taking this to the first equation, gives $1=b_{ii}+0 \Rightarrow b_{ii}=1$. It is now clear that $B=E_{ij}(\alpha)^{-1}$ is of the form described above.
\end{proof}
\begin{corollary}
Let $A$ be any $n \times n$ matrix. Then multiplying $A$ from the left by any $E_{ij}(\alpha)^{-1}$, of the same dimensions, yields a result matrix, $B=E_{ij}(\alpha)^{-1}A$, whose rows are $$
B_k=\begin{cases}
		A_i-\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
\end{corollary}
\begin{proof}
From ~\ref{cor:inv.elem.oper.row.add}, we have that $E_{ij}(\alpha)^{-1}A=E_{ij}(-\alpha)A$, and from ~\ref{prop:elem.oper.row.add}, the result above is immediate.
\end{proof}
\begin{proposition} \label{prop:pow.elem.oper.row.add}
Let $E_{ij}(\alpha)$ as defined above. then, for any $m \in \mathbb{N}$, we have $E_{ij}(\alpha)^m=E_{ij}(m\alpha)$
\end{proposition}
\begin{proof}
By induction on $m$.
For $m=1$, $E_{ij}(\alpha)^1=E_{ij}(1\alpha)$. For $m>1$, we have $E_{ij}(\alpha)^m=E_{ij}(\alpha)E_{ij}(\alpha)^{m-1}$. By the induction hypothesis, $E_{ij}(\alpha)^{m-1}=E_{ij}((m-1)\alpha)$. We denote $A=E_{ij}((m-1)\alpha)$. By proposition ~\ref{prop:elem.oper.row.add}, $B=E_{ij}(\alpha)A$ is the matrix whose rows are
$$
B_k=\begin{cases}
		A_i+\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
So $b_{ii}=a_{ii}+\alpha a_{ji}=1+\alpha 0=1+0=1$, and $b_{ij}=a_{ij}+\alpha a_{jj}=(m-1)\alpha+\alpha 1=(m-1+1)\alpha=m\alpha$, which proves the induction step.
\end{proof}
\newpage
\textbf{Corollary 1.5} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m \in \mathbb{N},(E_{i,j}^{-1})^m=(a_{l,k})$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=-m,i<j$, and all other elements are zero
\begin{proof}
By induction on $m$. \newline
$(a_{l,k})=E_{i,j}^{-1}$
For $m=2$, we observe that $(E_{i,j}^{-1})^2=E_{i,j}^{-1} \times E_{i,j}^{-1}=(a_{l,k}) \times (a_{l,k})$, means that $E_{i,j}^{-1}$ operates on itself as the row addition $R_i \leftarrow R_i-R_j$ \newline
So, the product matrix $(b_{l,k})$ has $b_{i,i}=a_{i,i}-a_{j,i}=1-0=1$, and $b_{i,j}=a_{i,j}-a_{j,j}=-1-1=-2$, and all other elements are zero. \newline
Now, we prove for $m+1$ \newline
$(a_{l,k})=(E_{i,j}^{-1})^{m+1}=E_{i,j}^{-1} \times (E_{i,j}^{-1})^m$. But, from the induction assumption, $(b_{l,k})=(E_{i,j}^{-1})^m$, has $b_{l,l}=1,1 \leq l \leq n$, and $b_{i,j}=-m,i<j$, and all other elements are zero. \newline
So, $(a_{l,k})=E_{i,j}^{-1} \times (b_{i,j})$ is the row addition $R_i \leftarrow R_i-R_j$ on $(b_{i,j})$, which means, $a_{i,i}=b_{i,i}-b_{i,j}=1=0=1$, and $a_{i,j}=b_{i,j}-b_{j,j}=-m-1=-(m+1)$, and all the other elements are zero, thus, we prove the induction step. \newline
\end{proof}
\textbf{Corollary 1.6} 
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m,r \in \mathbb{Z},
(a_{l,k})=E_{i,j}^{m+r}=E_{i,j}^{r+m}$ is the matrix where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=m+r=r+m,i<j$, and all other elements are zero. \newline
This shows that multiplying integer powers of matrices, from the set $E_n$ (which means, adding their exponents), is equivalent to adding integer numbers, which means that we have a canonical bijection, $(\mathbb{Z},+) \leftrightarrow (E_{i,j}^{\mathbb{Z}},\cdot)$, for any two fixed indices $i<j$, where $1 \leftrightarrow E_{i,j}^1=E_{i,j}$, and $-1 \leftrightarrow E_{i,j}^{-1}$ \newline 
\newpage
\textbf{Proposition 1.7} \newline
Let $(a_{l,k})=E_{i,j},t \neq i<j,(b_{l,k})=E_{s,t},j \neq s<t \in E_n$, Then, \newline
$(c_{l,k})=E_{i,j} \cdot E_{s,t}=E_{s,t} \cdot E_{i,j}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{s,t}=1$, and, all other elements are zero.
\begin{proof}
As seen above, $E_{s,t}$ is operating from the left on $E_{i,j}$ as the addition $R_i,j \leftarrow R_i+R_j$, so, $(c_{l,k})$ is $E_{s,t}$, with row $j$ being added to row $i$. \newline
So, $c_{i,k}=b_{i,k}+b{j,k},1 \leq k \leq n$. But, since $s \neq j$ the only element in row $j$ of $(b_{l,k})$ which is not zero is $b_{j,j}=1$, and $b_{i,j}=0$, so $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$ \newline
Also, $b_{j,i}=0$ (it is below the main diagonal), so, $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$. \newline
It is easy to verify that all the other elements in row $i$ of $(c_{j,k})$ are zero, and that all the other rows of $(c_{l,k})$ remain the same as they are in $(b_{l,k})$ \newline
Also, it is easy to verify that, under the condition that $t \neq i$, the multiplication is commuting, and yields the same product matrix. 
\end{proof}
\textbf{Proposition 1.8} \newline
Let $(a_{l,k})=E_{i,j},i<j,(b_{l,k})=E_{j,r},j<r \in E_n$, Then, \newline
\textbf{1.8.1} $(c_{l,k})=E_{i,j} \cdot E_{j,r}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{j,r}=1$, and $c_{i,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}$ by $E_{j,r}$ is the addition on row $j$ to row $i$ of the matrix $E_{j,}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. \newline
\end{proof}
On the other hand, \newline
\textbf{1.8.2} $(d_{l,k})=E_{j,r} \cdot E_{i,j}$ is a matrix with $d_{l,l}=1,1 \leq l \leq n$, and $d_{i,j}=1$, and $c_{j,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}$ by $E_{s,t}$ is the addition on row $j$ to row $i$ of the matrix $E_{s,t}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.3} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(f_{l,k})=E_{i,j}^{-1} \cdot E_{j,r}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and $f_{i,r}=1$ and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}^{-1}$ by $E_{i,j}^{-1}$ is the subtraction of row $j$ from row $i$ of the matrix $E_{j,r}^{-1}$, which gives $f_{i,k}=d_{i,k}-d_{j,k},1 \leq k \leq n$, so $f_{i,i}=d_{i,i}-d_{j,i}=1-0=1$, and $f_{i,j}=d_{i,j}-d_{j,j}=0-1=-1$, and $f_{i,r}=d_{i,r}-d_{j,r}=0-(-1)=0+1=1$, and, it is easy to verify that all other $f_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.4} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(g_{l,k})=E_{j,r}^{-1} \cdot E_{i,j}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}^{-1}$ by $E_{j,r}^{-1}$ is the subtraction of row $r$ from row $j$ of the matrix $E_{i,j}^{-1}$, which gives $g_{i,k}=c_{i,k}-c_{j,k},1 \leq k \leq n$, so $g_{j,j}=c_{j,j}-c_{r,j}=1-0=1$, and $g_{i,j}=c_{i,j}-c_{r,j}=-1-0=-1$, and $g_{j,r}=c_{j,r}-c_{r,r}=0-1=-1$, and, it is easy to verify that all other $g_{j,k},g_{i,k}$ are zero. 
\end{proof}
\textbf{Corollary 1.9} 
Let $(a_{l,k})=E_{i,j},(b_{l,k})=E_{j,r}$, Then\newline
\textbf{1.9.1}
$(c_{l,k})=[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=(E_{i,j} \cdot E_{j,r}) \cdot (E_{i,j}^{-1} \cdot E_{j,r}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{i,j} \cdot E_{j,r}=I+F_{i,j}+F_{i,r}+F_{j,r} \newline
(g_{l,k})=E_{i,j}^-1 \cdot E_{j,r}^-1=I-F_{i,j}+F{i,r}-F_{j,r}$ \newline
So, in the product matrix, $(c_{l,k})$, $c_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $c_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $c_{j,r}$, but $c_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 1+1 \cdot -1+1 \cdot 1=1+(-1)+1=1-1+1=1$ \newline
Which means that $[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=I+F_{i,r}=E_{i,r}$
\end{proof}
\textbf{1.9.2}
$(d_{l,k})=[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=(E_{j,r} \cdot E_{i,j}) \cdot (E_{j,r}^{-1} \cdot E_{i,j}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{j,r} \cdot E_{i,j}=I+F_{i,j}+F_{j,r} \newline
(g_{l,k})=E_{j,r}^-1 \cdot E_{i,j}^-1=I-F_{i,j}-F_{j,r}$ \newline
So, in the product matrix, $(d_{l,k})$, $d_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $d_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $d_{j,r}$, but $d_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot 0+f_{i,j} \cdot g_{j,r}+0 \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 0+1 \cdot -1+0 \cdot 1=0+(-1)+0=0-1+0=-1$ \newline
Which means that $[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=I-F_{i,r}=E_{i,r}^{-1}$
\end{proof}
\end{document}
