\documentclass[12pt]{article}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{setspace}
\usepackage{xfrac}
\usepackage{yfonts}

\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }
 {
  (
  \alec_cycle:nn { #1 } { #2 }
  )
 }

\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2
 {
  \seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }
  \seq_use:Nn \l_alec_cycle_seq { #1 }
 }
\ExplSyntaxOff

\begin{document}
\textbf{1. The group $U_n$} \newline
\newline
\textbf{Proposition 1.1} \newline
Let $E_n=\{ E_{i,j} \}_{i<j}$ be the set of all $n \times n$ matrices, $(e_{l,k})$, \newline where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=1,i<j$, and all other elements are zero. That is, $E_{i,j}$ has $1$ only on the main diagonal, and in one element, anywhere above the main diagonal. Let $A$ be any $n \times n$ matrix. Then,\newline
Multiplying $A$ by $E_{i,j}$ (from the left), $E_{i,j} \times A$, is operating as performing the elementary operation $R_i \leftarrow R_i+R_j$ on $A$
\begin{proof}
$A=(a_{l,k}),B=(b_{l,k})=E_{i_j} \times A=(e_{l,k}) \times (a_{l,k})$ \newline
$b_{l,k}=\sum_{r=1}^n e_{l,r} \cdot a_{r,k}$ \newline
For all the rows, except for row $i$, $b_{l,k}=\sum_{r=1}^n e_{l,r} \cdot a_{r,k}=0+0+\dots+e_{l,l} \cdot a_{l,k}+0+0+\dots+0+0=1 \cdot a_{l,k}=a_{l,k}$ \newline
For row $i$, $b_{i,k}=\sum_{r=1}^n e_{i,r} \cdot a_{r,k}=0+0+\dots+e_{i,i} \cdot a_{i,k}+0+0+e_{i,j} \cdot a_{j,k}+0+0+\dots+0+0=1 \cdot a_{i,k}+1 \cdot a_{j,k}=a_{i,k}+a_{j,k}$ \newline
This shows that the multiplication preserves the rows of $A$, except for row $i$, which becomes the sum of rows $i,j$
\end{proof}
\textbf{Corollary 1.2} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$E_{i,j}^{-1}=(a_{l,k})$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=-1,i<j$, and all other elements are zero.
\begin{proof}
$(b_{l,k})=E_{i,j} \times (a_{l,k})$ \newline
Multiplying $(a_{l,k})$ by $E_{i,j}$ from the left is operating on $(a_{l,k})$ as a row addition, $R_i \leftarrow R_i+R_j$, as seen above. \newline 
For all $1 \leq k \leq n,b_{i,k}=a_{i,k}+a_{j,k}$ \newline
But, the only element in row $j$ that is not zero is $a_{j,j=1}$, so, $b_{i,j}=a_{i,j}+a_{j,j}= \newline -1+1=0$, and, for all the other columns, $a_{j,k}=0$, so $b_{i,i}=a_{i,i}+a_{j,i}=1+0=1$, and $b_{i,k}=a_{i,k}+a_{j,k}=0+0=0$, which means that $(b_{l,k})=I_n$ \newline 
Easy to verify that also $(a_{l,k}) \times E_{i,j}=I_n$, and that $(a_{l,k})$ is a unique inverse of $E_{i,j}$, since, suppose we have another inverse matrix, $M=E_{i,j}^{-1}$, then $(a_{l,k}) \times E_{i,j}=I_n=M \times E_{i,j} \Rightarrow ((a_{l,k}) \times E_{i,j}) \times M=(M \times E_{i,j}) \times M
\Rightarrow (a_{l,k}) \times E_{i,j} \times M=M \times E_{i,j} \times M
\Rightarrow (a_{l,k}) \times (E_{i,j} \times M)=M \times (E_{i,j} \times M) \Rightarrow (a_{l,k}) \times I_n=M \times I_n \Rightarrow (a_{l,k})=M$ \newline
So, $(a_{l,k})=E_{i,j}^{-1}$ is the unique inverse of $E_{i,j}$ \newline 
\end{proof}
\newpage
\textbf{Corollary 1.3} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, and $A=(a_{l,k})$ any $n \times n$ matrix. Then,\newline
$E_{i,j}^{-1}=(b_{l,k})$ operates on $A$ as the row addition $R_i \leftarrow R_i-R_j$.
\begin{proof}
As seen above, $E_{i,j}^{-1}=(b_{l,k})$ is a matrix where $b_{l,l}=1,1 \leq l \leq n$, and $b_{i,j}=-1,i<j$, and all other elements are zero. \newline
So, the multiplication $(c_{l,k})=E_{i,j}^{-1} \times A=(b_{l,k}) \times (a_{l,k})$ is exactly the same as $E_{i,j} \times A$, for all the rows except for row $i$ \newline
For row $i$, we have $c_{i,k},1 \leq k \leq n=\sum_{r=1}^n b_{i,r} \cdot a_{r,k}$ \newline
But, the only elements that are not zero, in row $i$ of $(b_{l,k})$ are $b_{i,i}=1$, and $b_{i,j}=-1$, so, $c_{i_k}=b_{i,1} \cdot a_{1,k}+b_{i,2} \cdot a_{2,k}+\dots+b_{i,i} \cdot a_{i,k}+\dots+b_{i,j} \cdot a_{j,k}+\dots+b_{i,n-1} \cdot a_{n-1,k}+b_{i,n} \cdot a_{n,k}=0 \cdot a_{1,k}+0 \cdot a_{2,k}+\dots+1 \cdot a_{i,k}+\dots+(-1) \cdot a_{j,k}+\dots+0 \cdot a_{n-1,k}+0 \cdot a_{n,k}=0+0+\dots+a_{i,k}+\dots+(-a_{j,k})=a_{i,k}-a_{j,k}$, which shows that in the product matrix, $(c_{l,k})$, $R_i$ turns into $R_i-R_j$ \newline
\end{proof}
\textbf{Proposition 1.4} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m \in \mathbb{N},E_{i,j}^m=(a_{l,k})$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=m,i<j$, and all other elements are zero.
\begin{proof}
By induction on $m$. \newline
For $m=2$, we observe that $E_{i,j}^2=E_{i,j} \times E_{i,j}$, which means that the multiplication from the left of $E_{i,j}$ by itself is operating on $E_{i,j}$ as the row addition $R_i \leftarrow R_i+R_j$, so, $a_{i,i}=a_{i,i}+a{j,i}=1+0=1$, and, $a_{i,j}=a_{i,j}+a_{j,j}=1+1=2$, and, all the other elements are zero (easy to verify). \newline 
So, $(a_{l,k})=E_{i,j}^2$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=2,i<j$, and all other elements are zero. \newline
Now, we prove for $m+1$ \newline
$(a_{l,k})=E_{i,j}^{m+1}=E_{i,j} \times E_{i,j}^m$. But, from the induction assumption, $(b_{l,k})=E_{i,j}^m$, $b_{l,l}=1,1 \leq l \leq n$, and $b_{i,j}=m,i<j$, and all other elements are zero. \newline
Multiplying from the left $(b_{l,k})$ by $E_{i,j}$ is operating as the row addition $R_i \leftarrow R_i+R_j$, so, $a_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and, $a_{i,j}=b_{i,j}+b_{j,j}=m+1$, and easy to verify that all the other elements are zero, thus, we prove the induction step. \newline
\end{proof}
\newpage
\textbf{Corollary 1.5} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m \in \mathbb{N},(E_{i,j}^{-1})^m=(a_{l,k})$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=-m,i<j$, and all other elements are zero
\begin{proof}
By induction on $m$. \newline
$(a_{l,k})=E_{i,j}^{-1}$
For $m=2$, we observe that $(E_{i,j}^{-1})^2=E_{i,j}^{-1} \times E_{i,j}^{-1}=(a_{l,k}) \times (a_{l,k})$, means that $E_{i,j}^{-1}$ operates on itself as the row addition $R_i \leftarrow R_i-R_j$ \newline
So, the product matrix $(b_{l,k})$ has $b_{i,i}=a_{i,i}-a_{j,i}=1-0=1$, and $b_{i,j}=a_{i,j}-a_{j,j}=-1-1=-2$, and all other elements are zero. \newline
Now, we prove for $m+1$ \newline
$(a_{l,k})=(E_{i,j}^{-1})^{m+1}=E_{i,j}^{-1} \times (E_{i,j}^{-1})^m$. But, from the induction assumption, $(b_{l,k})=(E_{i,j}^{-1})^m$, has $b_{l,l}=1,1 \leq l \leq n$, and $b_{i,j}=-m,i<j$, and all other elements are zero. \newline
So, $(a_{l,k})=E_{i,j}^{-1} \times (b_{i,j})$ is the row addition $R_i \leftarrow R_i-R_j$ on $(b_{i,j})$, which means, $a_{i,i}=b_{i,i}-b_{i,j}=1=0=1$, and $a_{i,j}=b_{i,j}-b_{j,j}=-m-1=-(m+1)$, and all the other elements are zero, thus, we prove the induction step. \newline
\end{proof}
\textbf{Corollary 1.6} 
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m,r \in \mathbb{Z},
(a_{l,k})=E_{i,j}^{m+r}=E_{i,j}^{r+m}$ is the matrix where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=m+r=r+m,i<j$, and all other elements are zero. \newline
This shows that multiplying integer powers of matrices, from the set $E_n$ (which means, adding their exponents), is equivalent to adding integer numbers, which means that we have a canonical bijection, $(\mathbb{Z},+) \leftrightarrow (E_{i,j}^{\mathbb{Z}},\cdot)$, for any two fixed indices $i<j$, where $1 \leftrightarrow E_{i,j}^1=E_{i,j}$, and $-1 \leftrightarrow E_{i,j}^{-1}$ \newline 
\newpage
\textbf{Proposition 1.7} \newline
Let $(a_{l,k})=E_{i,j},t \neq i<j,(b_{l,k})=E_{s,t},j \neq s<t \in E_n$, Then, \newline
$(c_{l,k})=E_{i,j} \cdot E_{s,t}=E_{s,t} \cdot E_{i,j}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{s,t}=1$, and, all other elements are zero.
\begin{proof}
As seen above, $E_{s,t}$ is operating from the left on $E_{i,j}$ as the addition $R_i,j \leftarrow R_i+R_j$, so, $(c_{l,k})$ is $E_{s,t}$, with row $j$ being added to row $i$. \newline
So, $c_{i,k}=b_{i,k}+b{j,k},1 \leq k \leq n$. But, since $s \neq j$ the only element in row $j$ of $(b_{l,k})$ which is not zero is $b_{j,j}=1$, and $b_{i,j}=0$, so $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$ \newline
Also, $b_{j,i}=0$ (it is below the main diagonal), so, $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$. \newline
It is easy to verify that all the other elements in row $i$ of $(c_{j,k})$ are zero, and that all the other rows of $(c_{l,k})$ remain the same as they are in $(b_{l,k})$ \newline
Also, it is easy to verify that, under the condition that $t \neq i$, the multiplication is commuting, and yields the same product matrix. 
\end{proof}
\textbf{Proposition 1.8} \newline
Let $(a_{l,k})=E_{i,j},i<j,(b_{l,k})=E_{j,r},j<r \in E_n$, Then, \newline
\textbf{1.8.1} $(c_{l,k})=E_{i,j} \cdot E_{j,r}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{j,r}=1$, and $c_{i,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}$ by $E_{j,r}$ is the addition on row $j$ to row $i$ of the matrix $E_{j,}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. \newline
\end{proof}
On the other hand, \newline
\textbf{1.8.2} $(d_{l,k})=E_{j,r} \cdot E_{i,j}$ is a matrix with $d_{l,l}=1,1 \leq l \leq n$, and $d_{i,j}=1$, and $c_{j,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}$ by $E_{s,t}$ is the addition on row $j$ to row $i$ of the matrix $E_{s,t}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.3} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(f_{l,k})=E_{i,j}^{-1} \cdot E_{j,r}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and $f_{i,r}=1$ and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}^{-1}$ by $E_{i,j}^{-1}$ is the subtraction of row $j$ from row $i$ of the matrix $E_{j,r}^{-1}$, which gives $f_{i,k}=d_{i,k}-d_{j,k},1 \leq k \leq n$, so $f_{i,i}=d_{i,i}-d_{j,i}=1-0=1$, and $f_{i,j}=d_{i,j}-d_{j,j}=0-1=-1$, and $f_{i,r}=d_{i,r}-d_{j,r}=0-(-1)=0+1=1$, and, it is easy to verify that all other $f_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.4} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(g_{l,k})=E_{j,r}^{-1} \cdot E_{i,j}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}^{-1}$ by $E_{j,r}^{-1}$ is the subtraction of row $r$ from row $j$ of the matrix $E_{i,j}^{-1}$, which gives $g_{i,k}=c_{i,k}-c_{j,k},1 \leq k \leq n$, so $g_{j,j}=c_{j,j}-c_{r,j}=1-0=1$, and $g_{i,j}=c_{i,j}-c_{r,j}=-1-0=-1$, and $g_{j,r}=c_{j,r}-c_{r,r}=0-1=-1$, and, it is easy to verify that all other $g_{j,k},g_{i,k}$ are zero. 
\end{proof}
\textbf{Corollary 1.9} 
Let $(a_{l,k})=E_{i,j},(b_{l,k})=E_{j,r}$, Then\newline
\textbf{1.9.1}
$(c_{l,k})=[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=(E_{i,j} \cdot E_{j,r}) \cdot (E_{i,j}^{-1} \cdot E_{j,r}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{i,j} \cdot E_{j,r}=I+F_{i,j}+F_{i,r}+F_{j,r} \newline
(g_{l,k})=E_{i,j}^-1 \cdot E_{j,r}^-1=I-F_{i,j}+F{i,r}-F_{j,r}$ \newline
So, in the product matrix, $(c_{l,k})$, $c_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $c_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $c_{j,r}$, but $c_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 1+1 \cdot -1+1 \cdot 1=1+(-1)+1=1-1+1=1$ \newline
Which means that $[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=I+F_{i,r}=E_{i,r}$
\end{proof}
\textbf{1.9.2}
$(d_{l,k})=[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=(E_{j,r} \cdot E_{i,j}) \cdot (E_{j,r}^{-1} \cdot E_{i,j}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{j,r} \cdot E_{i,j}=I+F_{i,j}+F_{j,r} \newline
(g_{l,k})=E_{j,r}^-1 \cdot E_{i,j}^-1=I-F_{i,j}-F_{j,r}$ \newline
So, in the product matrix, $(d_{l,k})$, $d_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $d_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $d_{j,r}$, but $d_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot 0+f_{i,j} \cdot g_{j,r}+0 \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 0+1 \cdot -1+0 \cdot 1=0+(-1)+0=0-1+0=-1$ \newline
Which means that $[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=I-F_{i,r}=E_{i,r}^{-1}$
\end{proof}
\end{document}