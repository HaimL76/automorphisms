\documentclass[12pt]{article}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{amsfonts, amssymb}
\usepackage{mathrsfs, mathdots} 
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{setspace}
\usepackage{xfrac}
\usepackage{yfonts}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}

\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }
 {
  (
  \alec_cycle:nn { #1 } { #2 }
  )
 }

\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2
 {
  \seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }
  \seq_use:Nn \l_alec_cycle_seq { #1 }
 }
\ExplSyntaxOff

\begin{document}

\textbf{Notations}\newline
\textbf{$\mathbb{F}$}, an arbitrary field. \newline
\textbf{$F_{ij}(\alpha)$}, where $\alpha \in \mathbb{F}$, and $i<j$, is a $n \times n$ matrix, satisfying, 
$$
F_{ij}(\alpha)=(a_{kl})=\begin{cases}
			\alpha, & k=i \land l=j\\
            0, & \text{otherwise}
		 \end{cases}
$$
Define $F_{ij}:=F{ij}(1)$ \newline
\textbf{$E_{ij}(\alpha)$}, where $\alpha \in \mathbb{F}$, and $i<j$, is a $n \times n$ matrix, satisfying, 
$$
E_{ij}(\alpha)=(a_{kl})=\begin{cases}
            1, & k=l\\
			\alpha, & k=i \land l=j\\
            0, & \text{otherwise}
		 \end{cases}
$$
Define $E_{ij}:=E{ij}(1)$ \newline

\section{The group $U_n$}
%\textbf{1. The group $U_n$} 

\begin{proposition} \label{prop:elem.oper.row.add}
Let $A$ be any $n \times n$ matrix. Then multiplying $A$ from the left by any $E_{ij}(\alpha)$, of the same dimensions, yields a result matrix, $B=E_{ij}(\alpha)A$, whose rows are $$
B_k=\begin{cases}
		A_i+\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
In words, all the rows of $B$ are the rows of $A$, except for row $i$ of $B$, which is the addition of row $i$ of $A$ and the multiplication of row $j$ of A by the scalar $\alpha$.
\end{proposition}
\begin{proof}
Let the elements of $A$ be $(a_{kl})$, and the elements of $E_{ij}(\alpha)$ be $(e_{kl})$. Set the result matrix $B=E_{ij}(\alpha)A$, and let its elements be $(b_{kl})$. For each cell $b_{kl}=\sum_{r=1}^n e_{kr}a_{rl}$. For $k=i$, the sum, for each column $l$, is $0+\dots+0+e{ii}a_{il}+0+\dots+0+e_{ij}a_{jl}+0\dots+0=1a_{il}+\alpha a_{jl}=a_{il}+\alpha a_{jl}$, which proves that $B_i=A_i+\alpha A_j$. The rest is obvious as well.
\end{proof}
\begin{corollary}
\label{cor:inv.elem.oper.row.add}
Let $E_{ij}(\alpha)$ be a matrix of the described form. Then $E_{ij}(\alpha)$ has an inverse matrix, and its inverse is $E_{ij}(\alpha)^{-1}=E_{ij}(-\alpha)$
\end{corollary}
\begin{proof}
Easy to observe that $|E_{ij}(\alpha)|=1\neq 0$, so there exists a matrix $B=E_{ij}(\alpha)^{-1}$. Let the elements of $E_{ij}(\alpha)$ be $(e_{kl})$, and let the elements of $B$ be $(b_{kl})$. From proposition~\ref{prop:elem.oper.row.add}, we know that multiplying $B$ from the left by $E_{ij}(\alpha)$ yields a matrix $C$ with all the rows identical to the rows of $B$, except for row $i$, which is the addition $B_i+\alpha B_j$. Let the elements of $C$ be $(c_{kl})$. But $B=E_{ij}(\alpha)^{-1}$, which means $C=I_n$, so 
$$
c_{kl}=\begin{cases}
		1, & k=l\\
            0, & \text{otherwise}
		 \end{cases}
$$
This yields the following equations,
$$
\begin{cases}
c_{ii}=1=b_{ii}+\alpha b_{ji}\\
c_{ij}=0=b_{ij}+\alpha b_{jj}\\
		 \end{cases}
$$
But, $C_j=B_j$, which means that $c_{jj}=1=b_{jj}$. Taking this to the second equation, gives $0=b_{ij}+\alpha 1 \Rightarrow b_{ij}=\alpha$. Similarly, $c_{ji}=0=b_{ji}$. Taking this to the first equation, gives $1=b_{ii}+0 \Rightarrow b_{ii}=1$. It is now clear that $B=E_{ij}(\alpha)^{-1}$ is of the form described above.
\end{proof}
\begin{corollary}
Let $A$ be any $n \times n$ matrix. Then multiplying $A$ from the left by any $E_{ij}(\alpha)^{-1}$, of the same dimensions, yields a result matrix, $B=E_{ij}(\alpha)^{-1}A$, whose rows are $$
B_k=\begin{cases}
		A_i-\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
\end{corollary}
\begin{proof}
From ~\ref{cor:inv.elem.oper.row.add}, we have that $E_{ij}(\alpha)^{-1}A=E_{ij}(-\alpha)A$, and from ~\ref{prop:elem.oper.row.add}, the result above is immediate.
\end{proof}
\begin{proposition} 
\label{prop:pow.elem.oper.row.add}
Let $E_{ij}(\alpha)$ as defined above. then, for any $m \in \mathbb{N}$, we have $E_{ij}(\alpha)^m=E_{ij}(m\alpha)$
\end{proposition}
\begin{proof}
By induction on $m$.
For $m=1$, $E_{ij}(\alpha)^1=E_{ij}(1\alpha)$. For $m>1$, we have $E_{ij}(\alpha)^m=E_{ij}(\alpha)E_{ij}(\alpha)^{m-1}$. By the induction hypothesis, $E_{ij}(\alpha)^{m-1}=E_{ij}((m-1)\alpha)$. We denote $A=E_{ij}((m-1)\alpha)$. By proposition ~\ref{prop:elem.oper.row.add}, $B=E_{ij}(\alpha)A$ is the matrix whose rows are
$$
B_k=\begin{cases}
		A_i+\alpha A_j, & k=i\\
            A_k, & \text{otherwise}
		 \end{cases}
$$
So $b_{ii}=a_{ii}+\alpha a_{ji}=1+\alpha 0=1+0=1$, and $b_{ij}=a_{ij}+\alpha a_{jj}=(m-1)\alpha+\alpha 1=(m-1+1)\alpha=m\alpha$, which proves the induction step.
\end{proof}
\begin{corollary}
\label{cor:inv.pow.elem.oper.row.add}
Let $E_{ij}(\alpha)$ be a matrix as defined above. Then, for any $m \in \mathbb{N}$, $E_{ij}(\alpha)^{-m}=(E_{ij}(\alpha)^m)^{-1}=(E_{ij}(\alpha)^{-1})^m=E_{ji}(-m\alpha)$
\end{corollary}
\begin{proof}
By ~\ref{cor:inv.elem.oper.row.add}, $E_{ij}(\alpha)^{-1}=E_{ij}(-\alpha)$, and by ~\ref{prop:pow.elem.oper.row.add}, ($E_{ij}(\alpha)^{-1})^m=E_{ij}(-\alpha)^m=E_{ij}(m\cdot -\alpha)=E_{ij}(-m\alpha)$. We can commute the operations, to obtain exactly the same result.
\end{proof}
\begin{corollary}
Let $E_{ij}(\alpha), E_{ij}(\beta)$ be two matrices as defined above. Then, for any $m,s \in \mathbb{Z}$, $E_{ij}(\alpha)^m E_{ij}(\beta)^s=E_{ij}(m\alpha+s \beta)$
\end{corollary}
\begin{proof}
Set $A=E_{ij}(\alpha)^m$, and let $(a_{kl})$ be its elements.
If $m>0$, then by ~\ref{prop:pow.elem.oper.row.add}, $A=E_{ij}(\alpha)^m=E_{ij}(m\alpha)$. If $m<0$, then by ~\ref{cor:inv.pow.elem.oper.row.add}, we get the same. If $m=0$, then $A=E_{ij}(\alpha)^0=I_n$, But the unit matrix has $0$ everywhere, except for the main diagonal, which means that also $a_{ij}=0$, which means that $A=E_{ij}(\alpha)^0=I_n=E_{ij}(0)=E_{ij}(0\alpha)$, which proves that $E_{ij}(\alpha)^m=E_{ij}(m\alpha)$, for any $m \in \mathbb{Z}$. So, $E_{ij}(\alpha)^m E_{ij}(\beta)^s=E_{ij}(m\alpha) E_{ij}(s\beta)$, and the rest can be concluded from ~\ref{prop:elem.oper.row.add}.
\end{proof}
\begin{corollary}
Let $A$ be an upper triangular $n \times n$ matrix, over $\mathbb{Z}$, with $1$ on the main diagonal.
\end{corollary}
\begin{proof}
Set $A=E_{ij}(\alpha)^m$, and let $(a_{kl})$ be its elements.
If $m>0$, then by ~\ref{prop:pow.elem.oper.row.add}, $A=E_{ij}(\alpha)^m=E_{ij}(m\alpha)$. If $m<0$, then by ~\ref{cor:inv.pow.elem.oper.row.add}, we get the same. If $m=0$, then $A=E_{ij}(\alpha)^0=I_n$, But the unit matrix has $0$ everywhere, except for the main diagonal, which means that also $a_{ij}=0$, which means that $A=E_{ij}(\alpha)^0=I_n=E_{ij}(0)=E_{ij}(0\alpha)$, which proves that $E_{ij}(\alpha)^m=E_{ij}(m\alpha)$, for any $m \in \mathbb{Z}$. So, $E_{ij}(\alpha)^m E_{ij}(\beta)^s=E_{ij}(m\alpha) E_{ij}(s\beta)$, and the rest can be concluded from ~\ref{prop:elem.oper.row.add}.
\end{proof}
\begin{proposition} \label{prop:elem.oper.row.add.mult.closed}
Let $E_{ij}(\alpha),E_{st}(\beta)$ be two matrices defined as above, that is, $\alpha,\beta \in \mathbb{F}$, and $i<j$, and $s<t$. Then, $C=E_{ij}(\alpha)E_{st}(\beta)$ is an upper triangular matrix over $\mathbb{F}$, with $1$ on the main diagonal.
\end{proposition}
\begin{proof}
Let $(a_{kl})$ be the elements of $E_{ij}(\alpha)$, and let $(b_{kl})$ be the elements of $E_{st}(\beta)$
From ~\ref{prop:elem.oper.row.add}, we have that $C_k=B_k$, for each row $k \neq i$, and $C_i=B_i+\alpha B_j$. But, for any index $r<i<j$, we have $b_{jr}=0$, so, the sum $c_{ir}=b_{ir}+\alpha b_{jr}$, which means that $$
c_{ir}=\begin{cases}
		0, & r<i\\
            1, & r=i\\
            b_{ir}, & i<r<j \\
            b_{ij}+\alpha, & r=j\\
            b_{ir}+b_{jr}, & j<r<n\\
		 \end{cases}
$$, which proves that $C$ is of the form described above.
\end{proof}
\begin{proposition}
Let $A$ be an upper triangular matrix over $\mathbb{F}$, with $1$ on the main diagonal. Then, $A$ can be decomposed into a product of matrices of the form $E_{ij}(\alpha)$
\end{proposition}
\begin{proof}
Suppose $A$ is an upper triangular matrix, with $1$ on the main diagonal. Let $(a_{kl})$ be the elements of $A$, and let $a_{i_0j_0}$ be an arbitrary element of $A$, such that $i_0<j_0$, and $a_{i_0j_0}=\alpha_0\neq 0$.
We look at the product $A^{(0)}=E_{i_0j_0}(-\alpha_0)A$. By ~\ref{cor:inv.elem.oper.row.add}, we have $A_{i_0}^{(0)}=A_{i_0}+\alpha_0 A_{j_0}$. Let $a_{kl}^{(0)}$ be the elements of $A^{(0)}$, so, $a_{i_0r}^{(0)}=a_{i_0r}+\alpha_0 a_{j_0r}$, for all $1\leq r\leq n$. So, we can easily verify that,
$$
a_{i_0r}^{(0)}=\begin{cases}
		0, & 1\leq r<i_0\\
            1, & r=i_0\\
            a_{i_0r}, & i_0<r<j_0\\
            0, & r=j_0\\
            a_{i_0r}-\alpha_0 a_{j_0r}, & j<r\leq n\\
		 \end{cases}
$$
Now, we can choose another element, $a_{i_1j_1}$, such that $i_1<j_1$, and $a_{i_1j_1}=\alpha_1\neq 0$, and calculate $A^{(1)}=E_{i_1j_1}A^{(0)}$, in the same way described above. We do this, until all the elements of $A$, above the main diagonal, are $0$, that means, until we obtain the unit matrix, $I_n$. Since $A$ is of a finite dimension, it is obvious that the number of its elements is finite, and therefore, the number of elements above the main diagonal is finite. So, the number of steps, to obtain $I_n$ from $A$ necessarily finite, which means that $I_n=A^{(m)}$, where $m<\infty$.
So, $I_n=A^{(m)}=E_{i_mj_m}A^{(m-1)}=E_{i_mj_m}E_{i_{m-1}j_{m-1}}A^{(m-2)}=\dots=E_{i_mj_m}E_{i_{m-1}j_{m-1}}\dots E_{i_1j_1}E_{i_0j_0}A$
\end{proof}
\begin{corollary}
The set of all upper triangular matrices over $\mathbb{F}$, with $1$ on the main diagonal, is a multiplicative group.
\end{corollary}
\begin{proof}
By ~\ref{prop:elem.oper.row.add.mult.closed}, we know that for every two matrices of the form $E_{ij}(\alpha)$, we get a product in the form of an upper triangular matrix, with $1$ on the main diagonal. So, if $A=E_{ij}(\alpha)E_{st}(\beta)$, and $B=E_{kl}E_{}$\newline

this set is closed under multiplication. The associativity is obvious, as the operation is matrix multiplication. The standard unit matrix, $I_n=E_{ij}(0)$ is also a neutral matrix for all matrices of this form. And, by ~\ref{cor:inv.elem.oper.row.add}, every $E_{ij}(\alpha)$ has its unique inverse matrix, $E_{ij}(\alpha)^{-1}$, of this form, which makes this set a group, under the multiplication operation.
\end{proof}
\begin{notation}
\end{notation}
\newpage
\textbf{Corollary 1.5} \newline
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m \in \mathbb{N},(E_{i,j}^{-1})^m=(a_{l,k})$, where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=-m,i<j$, and all other elements are zero
\begin{proof}
By induction on $m$. \newline
$(a_{l,k})=E_{i,j}^{-1}$
For $m=2$, we observe that $(E_{i,j}^{-1})^2=E_{i,j}^{-1} \times E_{i,j}^{-1}=(a_{l,k}) \times (a_{l,k})$, means that $E_{i,j}^{-1}$ operates on itself as the row addition $R_i \leftarrow R_i-R_j$ \newline
So, the product matrix $(b_{l,k})$ has $b_{i,i}=a_{i,i}-a_{j,i}=1-0=1$, and $b_{i,j}=a_{i,j}-a_{j,j}=-1-1=-2$, and all other elements are zero. \newline
Now, we prove for $m+1$ \newline
$(a_{l,k})=(E_{i,j}^{-1})^{m+1}=E_{i,j}^{-1} \times (E_{i,j}^{-1})^m$. But, from the induction assumption, $(b_{l,k})=(E_{i,j}^{-1})^m$, has $b_{l,l}=1,1 \leq l \leq n$, and $b_{i,j}=-m,i<j$, and all other elements are zero. \newline
So, $(a_{l,k})=E_{i,j}^{-1} \times (b_{i,j})$ is the row addition $R_i \leftarrow R_i-R_j$ on $(b_{i,j})$, which means, $a_{i,i}=b_{i,i}-b_{i,j}=1=0=1$, and $a_{i,j}=b_{i,j}-b_{j,j}=-m-1=-(m+1)$, and all the other elements are zero, thus, we prove the induction step. \newline
\end{proof}
\textbf{Corollary 1.6} 
Let $E_{i,j}=(e_{l,k}),i<j \in E_n$, Then,\newline
$\forall m,r \in \mathbb{Z},
(a_{l,k})=E_{i,j}^{m+r}=E_{i,j}^{r+m}$ is the matrix where $a_{l,l}=1,1 \leq l \leq n$, and $a_{i,j}=m+r=r+m,i<j$, and all other elements are zero. \newline
This shows that multiplying integer powers of matrices, from the set $E_n$ (which means, adding their exponents), is equivalent to adding integer numbers, which means that we have a canonical bijection, $(\mathbb{Z},+) \leftrightarrow (E_{i,j}^{\mathbb{Z}},\cdot)$, for any two fixed indices $i<j$, where $1 \leftrightarrow E_{i,j}^1=E_{i,j}$, and $-1 \leftrightarrow E_{i,j}^{-1}$ \newline 
\newpage
\textbf{Proposition 1.7} \newline
Let $(a_{l,k})=E_{i,j},t \neq i<j,(b_{l,k})=E_{s,t},j \neq s<t \in E_n$, Then, \newline
$(c_{l,k})=E_{i,j} \cdot E_{s,t}=E_{s,t} \cdot E_{i,j}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{s,t}=1$, and, all other elements are zero.
\begin{proof}
As seen above, $E_{s,t}$ is operating from the left on $E_{i,j}$ as the addition $R_i,j \leftarrow R_i+R_j$, so, $(c_{l,k})$ is $E_{s,t}$, with row $j$ being added to row $i$. \newline
So, $c_{i,k}=b_{i,k}+b{j,k},1 \leq k \leq n$. But, since $s \neq j$ the only element in row $j$ of $(b_{l,k})$ which is not zero is $b_{j,j}=1$, and $b_{i,j}=0$, so $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$ \newline
Also, $b_{j,i}=0$ (it is below the main diagonal), so, $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$. \newline
It is easy to verify that all the other elements in row $i$ of $(c_{j,k})$ are zero, and that all the other rows of $(c_{l,k})$ remain the same as they are in $(b_{l,k})$ \newline
Also, it is easy to verify that, under the condition that $t \neq i$, the multiplication is commuting, and yields the same product matrix. 
\end{proof}
\textbf{Proposition 1.8} \newline
Let $(a_{l,k})=E_{i,j},i<j,(b_{l,k})=E_{j,r},j<r \in E_n$, Then, \newline
\textbf{1.8.1} $(c_{l,k})=E_{i,j} \cdot E_{j,r}$ is a matrix with $c_{l,l}=1,1 \leq l \leq n$, and $c_{i,j}=1$, and $c_{j,r}=1$, and $c_{i,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}$ by $E_{j,r}$ is the addition on row $j$ to row $i$ of the matrix $E_{j,}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. \newline
\end{proof}
On the other hand, \newline
\textbf{1.8.2} $(d_{l,k})=E_{j,r} \cdot E_{i,j}$ is a matrix with $d_{l,l}=1,1 \leq l \leq n$, and $d_{i,j}=1$, and $c_{j,r}=1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}$ by $E_{s,t}$ is the addition on row $j$ to row $i$ of the matrix $E_{s,t}$, which gives $c_{i,k}=b_{i,k}+b_{j,k},1 \leq k \leq n$, so $c_{i,i}=b_{i,i}+b_{j,i}=1+0=1$, and $c_{i,j}=b_{i,j}+b_{j,j}=0+1=1$, and $c_{i,r}=b_{i,r}+b_{j,r}=0+1=1$, and, it is easy to verify that all other $c_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.3} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(f_{l,k})=E_{i,j}^{-1} \cdot E_{j,r}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and $f_{i,r}=1$ and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{j,r}^{-1}$ by $E_{i,j}^{-1}$ is the subtraction of row $j$ from row $i$ of the matrix $E_{j,r}^{-1}$, which gives $f_{i,k}=d_{i,k}-d_{j,k},1 \leq k \leq n$, so $f_{i,i}=d_{i,i}-d_{j,i}=1-0=1$, and $f_{i,j}=d_{i,j}-d_{j,j}=0-1=-1$, and $f_{i,r}=d_{i,r}-d_{j,r}=0-(-1)=0+1=1$, and, it is easy to verify that all other $f_{i,k}$ are zero. 
\end{proof}
\textbf{1.8.4} 
Let $(c_{l,k})=E_{i,j}^{-1},(d_{l,k})=E_{j,r}^{-1}$ \newline
$(g_{l,k})=E_{j,r}^{-1} \cdot E_{i,j}^{-1}$ is a matrix with $f_{l,l}=1,1 \leq l \leq n$, and $f_{i,j}=-1$, and $f_{j,r}=-1$, and, all other elements are zero.
\begin{proof}
The multiplication from the left of $E_{i,j}^{-1}$ by $E_{j,r}^{-1}$ is the subtraction of row $r$ from row $j$ of the matrix $E_{i,j}^{-1}$, which gives $g_{i,k}=c_{i,k}-c_{j,k},1 \leq k \leq n$, so $g_{j,j}=c_{j,j}-c_{r,j}=1-0=1$, and $g_{i,j}=c_{i,j}-c_{r,j}=-1-0=-1$, and $g_{j,r}=c_{j,r}-c_{r,r}=0-1=-1$, and, it is easy to verify that all other $g_{j,k},g_{i,k}$ are zero. 
\end{proof}
\textbf{Corollary 1.9} 
Let $(a_{l,k})=E_{i,j},(b_{l,k})=E_{j,r}$, Then\newline
\textbf{1.9.1}
$(c_{l,k})=[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=(E_{i,j} \cdot E_{j,r}) \cdot (E_{i,j}^{-1} \cdot E_{j,r}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{i,j} \cdot E_{j,r}=I+F_{i,j}+F_{i,r}+F_{j,r} \newline
(g_{l,k})=E_{i,j}^-1 \cdot E_{j,r}^-1=I-F_{i,j}+F{i,r}-F_{j,r}$ \newline
So, in the product matrix, $(c_{l,k})$, $c_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $c_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $c_{j,r}$, but $c_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 1+1 \cdot -1+1 \cdot 1=1+(-1)+1=1-1+1=1$ \newline
Which means that $[E_{i,j},E_{j,r}]= E_{i,j} \cdot E_{j,r} \cdot E_{i,j}^{-1} \cdot E_{j,r}^{-1}=I+F_{i,r}=E_{i,r}$
\end{proof}
\textbf{1.9.2}
$(d_{l,k})=[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=E_{i,r}$
\begin{proof}
By associativity, $E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=(E_{j,r} \cdot E_{i,j}) \cdot (E_{j,r}^{-1} \cdot E_{i,j}^{-1})$, and we have already calculated these matrix products. \newline
$(f_{l,k})=E_{j,r} \cdot E_{i,j}=I+F_{i,j}+F_{j,r} \newline
(g_{l,k})=E_{j,r}^-1 \cdot E_{i,j}^-1=I-F_{i,j}-F_{j,r}$ \newline
So, in the product matrix, $(d_{l,k})$, $d_{i,j}=\sum_{k=1}^n f_{i,k} \cdot g_{k,j}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,j}+f_{i,j} \cdot g_{j,j}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot -1+1 \cdot 1=-1+1=0$ \newline
So, $d_{i,j}$ is canceled by multiplication. Easy to verify that the same goes also for $d_{j,r}$, but $d_{i,r}=\sum_{k=1}^n f_{i,k} \cdot g_{k,r}=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot g_{i,r}+f_{i,j} \cdot g_{j,r}+f_{i,r} \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=0 \cdot 0+ \dots +0 \cdot 0+f_{i,i} \cdot 0+f_{i,j} \cdot g_{j,r}+0 \cdot g_{r,r}+0 \cdot 0 \dots 0 \cdot 0=1 \cdot 0+1 \cdot -1+0 \cdot 1=0+(-1)+0=0-1+0=-1$ \newline
Which means that $[E_{j,r},E_{i,j}]= E_{j,r} \cdot E_{i,j} \cdot E_{j,r}^{-1} \cdot E_{i,j}^{-1}=I-F_{i,r}=E_{i,r}^{-1}$
\end{proof}
\end{document}
