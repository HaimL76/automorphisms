\documentclass[12pt]{article}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{tikz-cd}
\usepackage{xcolor}
\usepackage{xparse}

\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }
 {
  (
  \alec_cycle:nn { #1 } { #2 }
  )
 }

\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2
 {
  \seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }
  \seq_use:Nn \l_alec_cycle_seq { #1 }
 }
\ExplSyntaxOff

\begin{document}


\textbf{Exercise}
Let \( \{ E_{i,j} \}_{i<j} \) be the set of all elementary matrices, of this form. \newline
Prove that \( E_{i,j}^{-1}=(b_{l,k}) \) is \( E_{i,j}=(a_{l,k}) \), when we substitute \( a_{i,j}=1 \) with \( b_{i,j}=-1 \) \newline

\textbf{Proof}
We can see that directly from the fact that if we multiply \( E_{i,j}^{-1} \) by \( E_{i,j} \) from the left 
then \( E_{i,j} \) is operating on \( E_{i,j}^{-1} \) by adding row \( j \) to row \(i \) \newline
So, in the product matrix, \( (c_{l,k}) \), in order to have \( 1 \) \newline 
on the main diagonal, we need them to exist on the main diagonal
of \( E_{i,j}^{-1} \), to begin with. Now, in order to have \( c_{i,j}=0 \), we need to have the addition of \( j \) to \( i \)
giving \( c_{i,j}=a_{i,j}+b_{i,j}=0 \Rightarrow b_{i,j}=-a_{i,j}=-1 \) \newline

\textbf{Exercise} Prove that if \( (a_{ij})=E_{i,j}, i<j \) is an elementary matrix, \newline
then \( \forall m \in \mathbb(N), E_{i,j}^m \) is \(E_{i,j} \), but with \( a_{ij}=m \) \newline
$$
	E_{i,j} = \begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & 1 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix}
	\quad
	$$

$$ E_{i,j}^2=E_{i,j} \cdot E_{i,j} $$

Since \( E_{i,j} \) is en elemntary matrix, then it operates on the right matrix
as an addition of row \( j \) to row \( i \) \newline

So, $$
	E_{i,j}^2 = \begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & 2 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix}
	\quad
	$$

We assume this is true for all \( E_{i,j}^m \), now we prove for \( E_{i,j}^{m+1} \)
$$
	E_{i,j}^{m+1} = E_{i,j} \cdot E_{i,j}^{m}=\begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & 1 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & 1 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix}^{m}
	\quad
	$$
(by the assumption)
$$
	=\begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & 1 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & m & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	0 & 0 & \ddots & m+1 & 0 \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
	\end{pmatrix}
	\quad
	$$

From these two exercises, we obtain an almost trivial corollary \newline
\textbf{Exercise} Prove that if \( (a_{ij})=E_{i,j}, i<j \) is an elementary matrix, \newline
then \( \forall m \in \mathbb(N), (E_{i,j}^{-1})^m=(E_{i,j}^m)^{-1}=E_{i,j}^{-m} \) is \(E_{i,j} \), but with \( a_{ij}=-m \) \newline \newline
\textbf{Proof} We immitate both proofs from above (we can either show how the power of \( m \) is operating on \( E_{i,j}^{-1} \), 
 or show how the inversion is operating on \( E_{i,j}^m \)). \newline

\newpage
\underline{\textbf{Commutators of elementary matrices}} \newline

Let \( \{ E_{i,j} \}_{i<j} \) be the set of all elementary matrices of this form. \newline

\textbf{Exercise}
\( (a_{l,m})=E_{i,j}^{-1} \) is the matrix with \( 1 \) on the main diagonal, and \( -1 \) in \( a_{i,j} \) \newline
\textbf{Proof}
We can see that directly from the fact that in order to have \newline \( (c_{l,m})=(a_{l,m}) \cdot (b_{l,m})=E_{i,j} \cdot E_{i,j}^{-1}=I \), \newline
we need to have \( c_{i,j}=0 \), which means that adding row \( j \) to row \( i \), in \( E_{i,j}^{-1} \) (by the left multiplication of \( E_{i,j} \)) \newline
must give \( a_{i,j}+b_{i,j}=c_{i,j}=0 \Rightarrow b_{i,j}=-a_{i,j}=-1 \) \newline

\textbf{Exercise}
\( [E_{i,j},E_{j,k}]=E_{i,k} \) \newline
\textbf{Proof}
\( E_{i,j} \) is operating from left on \( E_{j,k} \) by addition of row \( j \) to row \( i \),
so, the product matrix, \( (a_{l,m})=E_{i,j} \cdot E_{j,k} \) has \( 1 \) on the main diagonal and in \( a_{j,k},a_{i,j},a_{i,k} \) \newline
\( E_{i,j}^{-1} \) is operating from left on \( E_{j,k}^{-1} \) by subtraction of row \( j \) from row \( i \),
so, the product matrix, \( (b_{l,m})=E_{i,j}^{-1} \cdot E_{j,k}^{-1} \) has \( 1 \) on the main diagonal and in \( b_{i,k} \), \newline
and \( -1 \) in \( b_{j,k},b_{i,j} \) \newline
Multiplying \( (a_{l,m}) \cdot (b_{l,m}) \) yields a product matrix, \( (c_{l,m}) \) with \( 1 \) on the main diagonal, and, \newline
since \( a_{i,i}=a_{i,j}=a_{i,k}=1 \), with all other cells in row \( j \) being \(0 \),
and since  \( b_{i,k}=b_{k,k}=1 \), and \( b_{j,k}=-1 \), multiplying row \( (a_{l,m})_{i} \) by column \( (b_{l,m})_{k} \) yields
the value \( c_{i,k}=b_{i,k}+b_{j,k}+b_{k,k}=1-1+1=1 \) \newline
We can see that multiplying \( (a_{l,m})_{i} \cdot (b_{l,m})_{j} \)
yields \( c_{i,j}=a_{i,i} \cdot b_{i,j}+a_{i,j} \cdot b_{j,j}=1 \cdot -1+1 \cdot 1=1-1=0 \) \newline
And, we can see that multiplying \( (a_{l,m})_{j} \cdot (b_{l,m})_{k} \)
yields \( c_{j,k}=a_{j,j} \cdot b_{j,k}+a_{j,k} \cdot b_{k,k}=1 \cdot -1+1 \cdot 1=1-1=0 \)

\textbf{Conclusion} \newline
\( [E_{j,k},E_{i,j}]=E_{j,k} \cdot E_{i,j} \cdot E_{j,k}^{-1} \cdot E_{i,j}^{-1}=
((E_{i,j}^{-1})^{-1} \cdot (E_{j,k}^{-1})^{-1} \cdot E_{i,j}^{-1} \cdot E_{j,k}^{-1})^{-1}= 
(E_{i,j} \cdot E_{j,k} \cdot E_{i,j}^{-1} \cdot E_{j,k}^{-1})^{-1}=[E_{i,j},E_{j,k}]^{-1}  \) \newline

For example, \( n=4 \), \newline
$$ E_{1,2} \cdot E_{2,3}=\begin{pmatrix} 
	1 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & 0 & 0 \\
	0 & 1 & 1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & 1 & 1 & 0 \\
	0 & 1 & 1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} $$
$$  E_{1,2}^{-1} \cdot E_{2,3}^{-1}=\begin{pmatrix} 
	1 & -1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & 0 & 0 \\
	0 & 1 & -1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & -1 & 1 & 0 \\
	0 & 1 & -1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \newline
$$
$$  [E_{1,2} \cdot E_{2,3}]=E_{1,2} \cdot E_{2,3} \cdot E_{1,2}^{-1} \cdot E_{2,3}^{-1}=$$
$$=\begin{pmatrix} 
	1 & 1 & 1 & 0 \\
	0 & 1 & 1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & -1 & 1 & 0 \\
	0 & 1 & -1 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & 0 & 1 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
\end{pmatrix}=E_{1,3}$$

\textbf{Exercise}
\( [E_{i,j},E_{l,k}]=I \), where \( j \neq l \) \newline
\textbf{Proof}
\( E_{i,j} \) is operating from left on \( E_{l,k} \) by addition of row \( j \) to row \( i \),
so, the product matrix, \( (a_{n,m}=E_{i,j} \cdot E_{l,k} \) has \( 1 \) on the main diagonal and in \( a_{l,k},a_{i,j} \) \newline
\( E_{i,j}^{-1} \) is operating from left on \( E_{l,k}^{-1} \) by subtraction of row \( j \) from row \( i \),
so, the product matrix, \( (b_{n,m}=E_{i,j}^{-1} \cdot E_{l,k}^{-1} \) has \( 1 \) on the main diagonal, \newline
and \( -1 \) in \( b_{l,k},b_{i,j} \) \newline
We can see that multiplying \( (a_{n,m})_{i} \cdot (b_{n,m})_{j} \)
yields \( c_{i,j}=a_{i,i} \cdot b_{i,j}+a_{i,j} \cdot b_{j,j}=1 \cdot -1+1 \cdot 1=1-1=0 \) \newline
And, we can see that multiplying \( (a_{n,m})_{l} \cdot (b_{n,m})_{k} \)
yields \( c_{l,k}=a_{l,l} \cdot b_{l,k}+a_{l,k} \cdot b_{k,k}=1 \cdot -1+1 \cdot 1=1-1=0 \)


For example, \( n=4 \), \newline
$$ E_{1,2} \cdot E_{3,4}=\begin{pmatrix} 
	1 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} $$
$$  E_{1,2}^{-1} \cdot E_{3,4}^{-1}=\begin{pmatrix} 
	1 & -1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & -1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & -1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & -1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \newline
$$
$$  [E_{1,2} \cdot E_{3,4}]=E_{1,2} \cdot E_{3,4} \cdot E_{1,2}^{-1} \cdot E_{3,4}^{-1}=$$
$$=\begin{pmatrix} 
	1 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix} \cdot \begin{pmatrix} 
	1 & -1 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & -1 \\
	0 & 0 & 0 & 1 \\
	\end{pmatrix}=\begin{pmatrix} 
	1 & 0 & 0 & 0 \\
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 \\
\end{pmatrix}=I$$

\textbf{Conclusion} \newline
\( [E_{i,j},[E_{j,k},E_{k,l}]]=[E_{i,j},E_{j,l}]=E_{i,l} \) \newline
\( [E_{i,j},[E_{j,k},E_{m,l}]], m \neq k=[E_{i,l},I]=I \) \newline
\( [E_{i,m},[E_{j,k},E_{k,l}]], m \neq j=[E_{i,m},E_{j,l}]=I \) \newline

\( \Rightarrow [E_{i_{1},i_{2}},[E_{i_{3},i_{4}},...[E_{i_{n-2},i_{n-1}},E_{i_{n-1},i_{n}}]]=
\begin{cases}
    E_{i_{1},i_{n}},& i_{2k}=i_{2k+1},\forall 1 \leq k \leq \frac{n}{2}-1\\
    I,              & \text{otherwise}
\end{cases}
\)

\textbf{Exercise} \newline
\( \#\{ E_{i,j} \in M_{n}(\mathbb{Z}) \}_{i<j}={n \choose 2} \) \newline

\textbf{Proof} \newline
\( (a_{i,j}=E_{i,j}) \). We need to count the options for \( 1 \) above the main diagonal. \newline
\( a_{l,l}=1, \forall 1 \leq l \leq n \), so, if \( i=l \), we have \( n-l=n-i \) options to choose the column index \( j \). \newline
So, the total number of options for \( i,j \) is \( \sum_{k=1}^{n-1}=\frac{(1 + n-1) \cdot (n-1)}{2}=\frac{n \cdot (n-1)}{2}={n \choose 2} \) \newline

This means that we have \( {n \choose 2}^2 \) commutators of the form \( [E_{i,j},E_{l,k}] \).

\textbf{Exercise} \newline
\( \#\{ [E_{i,j},E_{l,k}] \neq I \in M_{n}(\mathbb{Z}) \}_{i<j}=2 \cdot {n \choose 3} \) \newline

\textbf{Proof} \newline
As shown above, \( [E_{i,j},E_{l,k}] \neq I \Leftrightarrow j = l \) \newline
Which means we're counting all the commutators of the form \( [E_{i,j},E_{j,k}] \). \newline
So, the count of such commutators is based on the number of options to choose \newline
ordered triples \( \{i,j,k\} \) out of the ordered set \( [n]=\{1,2,...,n\} \), which is \( {n \choose 3} \) \newline
But, as already shown above, \( [E_{l,k},E_{i,j}]=[E_{i,j},E_{l,k}]^{-1} \), so, for each triple \( \{i,j,k\} \), we have
two commutators, \( [E_{i,j},E_{j,k}] \) and its inverse, which sum up to \( {n \choose 3} \) pairs of commutators. \newline

For example, \( n=5 \), \newline
$$ (a_{l,k})=E_{i,j}=\begin{pmatrix} 
	1 & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} \\
	0 & 1 & a_{2,3} & a_{2,4} & a_{2,5} \\
	0 & 0 & 1 & a_{3,4} & a_{3,5} \\
	0 & 0 & 0 & 1 & a_{4,5} \\
	0 & 0 & 0 & 0 & 1 \\
\end{pmatrix} 
$$
Where \( a_{i,j}=1\), and all other \( a_{l,k}=0 \) \newline
The number of options for choosing \( i,j \), in this case, are \( 1+2+3+4=10={5 \choose 2} \), \newline
so, we have \( 10^2=100 \) commutators. 
The number of triples we can choose from \( [5]=\{1,2,3,4,5\} \) is \newline
\( \#\{\{1,2,3\},\{1,2,4\},\{1,2,5\},\{1,3,4\},\{1,3,5\},\{1,4,5\},\{2,3,4\},\{2,3,5\},\{2,4,5\},\{3,4,5\}\}=10={5 \choose 3} \), \newline
so we have 10 commutators that are not the unit matrix, and their inverse, total \( 20 = 2 \cdot 10=2 \cdot {5 \choose 3} \). \newline

\textbf{Exercise} \newline
Given the set of commutators of elementary matrices of the form \newline \( \{ [E_{i,j},E_{j,k}] \in M_{n}(\mathbb{Z}) \}_{i<j<k} \), \newline
we can divide this set to subsets of the form \newline \( \{ 
\{ [E_{i_{1},j_{1,1}},E_{j_{1,1},k_{1}}], [E_{i_{1},j_{1,2}},E_{j_{1,2},k_{1}}],..., [E_{i_{1},j_{1,l_{1}}},E_{j_{1,l_{1}},k_{1}}] \},..., \newline
\{ [E_{i_{m},j_{m,1}},E_{j_{m,1},k_{1}}],...,[E_{i_{m},j_{m,l_{m}}},E_{j_{m,l_{m}},k_{1}}]\}\} \) \newline 
These subsets are equivalence classes, trivially, since the relation is equality (i.e. \( [E_{i_{l},j_{l,m_{1}}},E_{j_{l,m_{1}},k_{l}}]=
[E_{i_{l},j_{l,m_{2}}},E_{j_{l,m_{2}},k_{l}}],i_{l}<j_{l,m_{1}},j_{l,m_{2}}<k_{l} \)). \newline
Fix \( {i,k},1 \leq i \leq n-1,3\ leq k \leq n \), then all the triples of the form \( \{i,j,k\},i \leq i+1 \leq k-1 \) are in the same equivalence class, \newline
due to the above equality. So, the number of these equivalence classes is \( 2 \cdot {n-1 \choose 2} \) \newline \newline
\textbf{Proof} \newline
By induction on \( n \). For \( n=3 \), we have only one triple, namely \( \{1,2,3\} \), so \( {3-1 \choose 2}={2 \choose 2}=1 \)\newline
For \( n+1 \), we shall observe that if we add one to the upper bound (i.e. \( n \rightarrow n'=n+1 \), \newline
then we add one more equivalence class, for each one of the lower bounds of \( n'-1=n \) (i.e., the index \( i \)). \newline
But we also add a new eqivalence class, whose lower bound is \( i=n+1-2=n-1=n'-2 \), which was not in any equivalence class \newline
for \( n=n'-1 \), since we consider only the triples where \( i \leq n-2 \). So, if we mark \( m_{n} \) as the number of equivalence classes \newline
for \( n \), then we have \( m_{n'}=m_{n+1}=m_{n}+(n-2)+1=m_{n}+n-1 \). But, by the assumption, \( m_{n}={n-1 \choose 2} \), \newline 
so \( m_{n'}=m_{n+1}=m_{n}+n-1={n-1 \choose 2}+n-1=\frac{(n-1) \cdot (n-2)}{2}+n-1=\frac{n^2-3n+2}{2}+n-1=\frac{n^2-3n+2+2n-2}{2}=
\frac{n^2-n}{2}=\frac{n \cdot (n-1)}{2}={n \choose 2}=m_{n+1}=m_{n'} \), and we proved the assumption \newline
 
\textbf{The group \( U_{n}(\mathbb{Z}) \)} \newline
We have proved several basic facts, regarding elementary matrices, of the form \( \{ E_{i,j} \}_{i<j} \). \newline
Now, we shall propose a few more basic facts. \newline
\textbf{Notation} \newline
We mark by \( U_{n}(\mathbb{Z}) \) the set of all upper triangular matrices \( n \times n \) with \( 1 \) in the main diagonal, 
and any integer values above the main diagonal, \( a_{i,j} \in \mathbb{Z} \). \newline

\textbf{Exercise} Prove that the set \( U_{n}(\mathbb{Z}) \) is a group, with the usual operation of matrix multiplication. \newline \newline
\textbf{Proof}
$$
(a_{i,j})=A=\begin{pmatrix} 
	1 & a_{1,2} & \dots & a_{1,n-1} & a_{1,n} \\
	0 & 1 & \dots & a_{2,n-1} & a_{2,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & a_{k,n-1} & a_{k,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 1 & a_{n-1,n} \\
	0 & 0 & \dots & 0 & 1 \\
\end{pmatrix},
(b_{i,j})=B=\begin{pmatrix} 
	1 & b_{1,2} & \dots & b_{1,n-1} & b_{1,n} \\
	0 & 1 & \dots & b_{2,n-1} & b_{2,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & b_{k,n-1} & b_{k,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 1 & b_{n-1,n} \\
	0 & 0 & \dots & 0 & 1 \\
\end{pmatrix}
$$
We need to prove that \( (c_{i,j})=A \cdot B \in U_{n}(\mathbb{Z}) \) \newline
I. \( 1 \leq l \leq n, c_{l,l}=\sum_{k=1}^{n} a_{l,k} \cdot b_{k,l} \). \newline
We observe that \( a_{l,1}=a_{l,2}= \dots =a_{l,l-1}=0 \), and \( b_{l+l,l}=b_{l+2,l}= \dots =b_{n,l}=0 \). \newline
So, \( c_{l,l}=\sum_{k=1}^{n} a_{l,k} \cdot b_{k,l}=0+0+ \dots +0+a_{l,l} \cdot b_{l,l}+0+0 \dots 0=1 \). \newline
This proves that each element \underline{on} the main diagonal of \( (c_{i,j}) \) is \( 1 \). \newline \newline

II. \( 2 \leq l \leq n,1 \leq m \leq l-1, c_{l,m}=\sum_{k=1}^{n} a_{l,k} \cdot b_{k,m} \). \newline
We observe that \( a_{l,1}=a_{l,2}= \dots =a_{l,l-1}=0 \), and \( b_{m+1,m}=b_{m+2,m}= \dots =b_{n,m}=0 \). \newline
This means, that \( a_{l,k} \cdot b_{k,m}=0, 1 \leq k \leq l-1 \), \newline
because the first \( l-1 \) elements of \( a_{l,k} \) are \( 0 \). \newline
and the last  \( n-m \) elements of \( b_{k,m} \) are also \( 0 \). \newline
This proves that each element \underline{under} the main diagonal of \( (c_{i,j}) \) is \( 0 \). \newline

III. \( 2 \leq l \leq n,1 \leq m \leq l-1, c_{m,l}=\sum_{k=1}^{n} a_{m,k} \cdot b_{k,l} \). \newline
\( a_{m,k},b_{k,l} \in \mathbb{Z} \Rightarrow \sum_{k=1}^{n} a_{l,k} \cdot b_{k,m} \in \mathbb{Z} \). \newline
This proves the each element \underline{above} the main diagonal of \( (c_{i,j}) \) is an integer. \newline

Thus, we prove that \( U_{n}(\mathbb{Z}) \) is closed under matrix multiplication. \newline

Associativity is obvious, from the fact that matrix multiplication is associative. \newline

Obviously, \( I_{n} \) is a matrix of this form, so the unit of \( U_{n}(\mathbb{Z}) \) is \( I_{n} \). \newline

The fact that all matrices of this form have an inverse is obvious by looking at the rank of
a matrix of this form, which, clearly, is \( n \), since the matrix is already in a reduced form. \newline

Conclusion: \( U_{n}(\mathbb{Z}) \) is a group. \newline

\textbf{Exercise} All the matrices of the form \( \{ E_{i,i+1}, 1 \leq i \leq n-1 \} \) generate \( U_{n}(\mathbb{Z}) \). \newline \newline
\textbf{Proof} \newline
We already proved that \( [E_{i,j},E_{j,k}]=E_{i,k} \), specifically, \( [E_{i,i+1},E_{i+1,i+2}]=E_{i,i+2} \). \newline
Also, we proved that \( \forall m \in \mathbb{N}, (a_{i,j})^m=E_{i,j}^m=(a_{i,j}) \) with \( a_{i,j}=m \). \newline
This mean that all the matrices of the form \( \{ E_{i,i+1} \} \) yield all the matrices which have only one integer above 
the main diagonal. \newline

Let
$$
(a_{i,j})=A=\begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & a_{i,j} & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
\end{pmatrix},
(b_{l,k})=B=\begin{pmatrix} 
	1 & 0 & \dots & 0 & 0 \\
	0 & 1 & \dots & 0 & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & b_{l,k} & 0 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 1 & 0 \\
	0 & 0 & \dots & 0 & 1 \\
\end{pmatrix}
$$
\( |a_{i,j}|,|b_{l,k}|>1 \), and the two pairs of indices, \( \{i,j\},\{l,k\} \), are pairwise disjoint. \newline
Then, \( (c_{m,p})=A \cdot B \) is the matrix with \( 1 \) on the main diagonal, \( 0 \) below the main diagonal,\newline
and \( c_{i,j}=a_{i,j},c_{l,k}=b_{l,k} \). \newline
To prove this, we can observe that since \( |a_{i,j}|,|b_{l,k}|>1 \), we can write (based on what we already proved) \newline
\( (a_{i,j})=A=E_{i,j}^{a_{i,j}} \) \newpage

\textbf{The free Lie algebra \( L_{n}\)} \newline
The group \( U_{n}(\mathbb{Z}) \) gives rise to a Lie algebra over some field \( \mathbb{F} \), \newline
which consists of all matrices of the form $$ (a_{l,k})=\begin{pmatrix} 
	0 & a_{1,2} & a_{1,3} & \dots & a_{1,n} \\
	0 & 0 & a_{2,3} & \dots & a_{2,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & a_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix} 
$$
Which means, we have \( 0 \) on the main diagonal and under it, and \( a_{i,j} \in \mathbb{F},i<j \), \newline
that is, arbitrary scalars above the main diagonal. These matrices are precisely the matrices of \( U_{n}(\mathbb{Z}) \) \newline
minus the unit matrix, and with scalars from \( \mathbb{F} \), rather than \( \mathbb{Z} \). \newline

\textbf{Exercise} This is a Lie algebra over \( \mathbb{F} \) \newline
\textbf{Proof} \newline
Defining the multiplication (Lie brackets) on \( L_{n} \) by the standard matrix commutator
(i.e., \( \forall A,B \in M_{n}(\mathbb{F}),[A,B] := A \cdot B-B \cdot A \), automatically
gives all the Lie algebra axioms, for example,\newline
\( \forall A,B,C \in M_{n}(\mathbb{F}),\alpha,\beta \in \mathbb{F},\newline
[\alpha \cdot A+\beta \cdot B,C]=(\alpha \cdot A+\beta \cdot B) \cdot C-C \cdot \alpha \cdot A+\beta \cdot B)=\newline
(\alpha \cdot A) \cdot C+(\beta \cdot B) \cdot C-(C \cdot (\alpha \cdot A)+C \cdot (\beta \cdot B)=\newline
\alpha \cdot (A \cdot C)+\beta \cdot (B \cdot C)-\alpha \cdot (C \cdot A)-\beta \cdot (C \cdot B)=\newline
\alpha \cdot (A \cdot C)-\alpha \cdot (C \cdot A)+\beta \cdot (B \cdot C)-\beta \cdot (C \cdot B)=\newline
\alpha \cdot (A \cdot C-C \cdot A)+\beta \cdot (B \cdot C-C \cdot B)=\alpha \cdot [A,C]+\beta \cdot [B,C]\newline
\), Thus we prove linearity in the first component. There should be no need to prove all other axioms, \newline
since they can all be easily proved using the same matrix and scalar multiplications. \newline
We do need to prove that \( L \) is closed under Lie brackets defined above. \newline
Let $$ A=(a_{l,k})=\begin{pmatrix} 
	0 & a_{1,2} & a_{1,3} & \dots & a_{1,n} \\
	0 & 0 & a_{2,3} & \dots & a_{2,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & a_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix},B=(b_{l,k})=\begin{pmatrix} 
	0 & b_{1,2} & b_{1,3} & \dots & b_{1,n} \\
	0 & 0 & b_{2,3} & \dots & b_{2,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & b_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix}
$$
Then,
$$ A \cdot B=\begin{pmatrix} 
	0 & 0 & a_{1,2} \cdot b_{2,3} & \dots & \sum_{k=2}^{n-1} a_{1,k} \cdot b_{k,n} \\
	0 & 0 & 0 & \dots & \sum_{k=3}^{n-1} a_{2,k} \cdot b_{k,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & a_{n-1,n} \cdot b_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix} $$
$$ B \cdot A=\begin{pmatrix} 
	0 & 0 & b_{1,2} \cdot a_{2,3} & \dots & \sum_{k=2}^{n-1} b_{1,k} \cdot a_{k,n} \\
	0 & 0 & 0 & \dots & \sum_{k=3}^{n-1} b_{2,k} \cdot a_{k,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & b_{n-1,n} \cdot a_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix}
$$
And then,
$$ [A,B]=A \cdot B-B \cdot A=\begin{pmatrix} 
	0 & 0 & a_{1,2}-b_{1,2} \cdot a_{2,3} & \dots & \sum_{k=2}^{n-1} a_{1,k} \cdot b_{k,n}-\sum_{k=2}^{n-1} b_{1,k} \cdot a_{k,n} \\
	0 & 0 & 0 & \dots & \sum_{k=3}^{n-1} a_{2,k} \cdot b_{k,n}-\sum_{k=3}^{n-1} b_{2,k} \cdot a_{k,n} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & 0 & a_{n-1,n} \cdot b_{n-1,n}-b_{n-1,n} \cdot a_{n-1,n} \\
	0 & 0 & 0 & \dots & 0 \\
\end{pmatrix}
$$
So, the Lie brackets operation, defined above, is preserving the form of the matrices in \( L_{n} \). \newline
In addition, we observe that the second diagonal, above the main diagonal, is also \( 0\), thus we conclude \newline
that \( \forall A,B,C,D \in L_{n},[[A,B],[C,D]] \) will have the main, the second and the third diagonal all \( 0 \). \newline
In other words, \( L_{n} \) is a nilpotent Lie algebra. \newline

After verifying that the Lie brackets operation is defined, as multiplication in \( L_{n} \), we observe also that \( L_{n} \) \newline
is closed under addition, defined as the standard matrix addition. But this is obvious, since the matrix addition is simply \( a_{i,j}+b_{i,j},1 \leq i,j \leq n \), \newline 
and so, the sums of all \( 0 \) on the main diagonal and under it, which means \( a_{i,j}+b_{i,j},j \leq i=0+0=0 \)\newline
And the sums of all elements above the main diagonal, \( a_{i,j}+b_{i,j},j > i \), are sums of two scalars in \( \mathbb{F} \),. \newline
hence, scalars in the same field. \newline
in the same way, \( L_{n} \) being closed under multiplication by scalar is also a trivial fact. \newline

\textbf{Exercise} The Lie algebra \( L_{n} \) has a base, which is all the elementary matrices \newline
of the form \( E_{i,i+1} \) minus the unit matrix, that is, \( e_{i,i+1}=E_{i,i+1}-I \). \newline

\textbf{Exercise} The Lie algebra $L_n$, has an automorphism which acts on the base elements as the permutation \newline
\( \cycle{e_{1,2},e_{n-1,n}} \cdot \cycle{e_{2,3},e_{n-2,n-1}} \dots \cycle{e_{i,i+1},e_{n-i,n-i+1}} \dots \) \newline
This automorphism is of the form,
$1 \leq i < j \leq n,\eta(e_{i,j}):=-e_{n-i,n-i+1}$  \newline
\textbf{Proof} \newline
We prove that $\eta$ is a homomorphism. \newline
$\eta(e_{i,i+1})=\eta(e_{i,i+1}) \cdot \eta(e_{i,i+1})$

\end{document}
