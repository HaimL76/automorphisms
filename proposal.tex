\documentclass[12pt]{article}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{amsfonts, amssymb}
\usepackage{mathrsfs, mathdots} 
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{setspace}
\usepackage{xfrac}
\usepackage{yfonts}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notations}[theorem]{Notations}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }
 {
  (
  \alec_cycle:nn { #1 } { #2 }
  )
 }

\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2
 {
  \seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }
  \seq_use:Nn \l_alec_cycle_seq { #1 }
 }
\ExplSyntaxOff

\begin{document}
\begin{abstract}
Let $G$ be any group. For any natural number $n\in\mathbb{N}$, let $a_n$ be the number of subgroups $H\leq G$, such that $[G:H]=n$. Assume $G$ is finitely-generated, then $a_n<\infty$, and we can define a $\zeta$-function of the form $\zeta_G(s):=\sum_{i=1}^\infty a_nn^{-s}$, where $s\in\mathbb{C}$. Assume, in addition, that $G$ is also nilpotent and torsion-free, then this function has properties of the Riemann $\zeta$-function, mainly the decomposition of $\zeta$ to an Euler product of local factors indexed by primes. 
Using different variations of the $\zeta$-function and its factorization, we can obtain more information about $G$ and specific subgroups of $G$. Specifically, we are interested in the number of pro-isomorphic subgroups of $G$, and in this research we shall display an approach to the problem of counting them. 
\end{abstract}
\section{Scientific Background}
\subsection{Introduction}
We start our discussion by the basic fact, that for any finitely generated group, $G$, and for any natural number, $n\in\mathbb{N}$ number, there is a finite number of subgroups $H\leq G$, such that $[G:H]=n$.
\begin{proof}
Let $H\leq G$, such that $[G:H]=n$, then $\sfrac{G}{H}:=\{g_1H,g_2H,\dots,g_nH\}$ is the set containing all left cosets of $H$. We shall define an operation $*:G\times\sfrac{G}{H}\rightarrow \sfrac{G}{H}$, in the following way. $\forall g\in G$, and $\forall g_iH\in\sfrac{G}{H}$, the operation is $g*g_iH:=(gg_i)H=g_jH$, that is, $g$ maps a left cost to another left coset. But that means that $g$ maps every index $i\in [n]$ to another index, which means that $g$ operates as a permutation on $[n]$, so $*$ defines a homomorphism $f:G\rightarrow\mathcal{S}_n$, from $G$ to the symmetric group of order $n$. $H$ is a subgroup, so $\forall g\in G$, it is clear that $g\in H$ iff $gH=H$. Assume that $i_0$  is the index of the left coset which identifies with $H$, i.e. $g_{i_0}H=H$, then $g\in H$ iff $g*g_{i_0}H=H$, which means that the permutation $f(g)$ stabilizes $i_0$, i.e. $f(g)(i_0)=i_0$. So, we can write $H=\{g\in G : f(g)(i_0)=i_0\}$. From this observation, it is clear that $\#\{H\leq G : [G:H]=n\}\leq\#\{f:G\rightarrow \mathcal{S}_n\}$. But all $f$ are homomorphisms from a finitely generated group to a finite group, and since group homomorphisms are uniquely determined by the mapping of the generators, it is clear that $\#\{f:G\rightarrow \mathcal{S}_n\}<\infty$, which proves the proposition.
\end{proof}
After proving the fact above, we know that some types of subgroups are countable, because we can divide all the subgroups $H<G$, of a finitely generated group, to infinitely many equivalence classes of subgroups of index $n$, for every $n\in\mathbb{N}$, each class of finite cardinality, and index the subgroups in each class by some arbitrary order, thus making the set of all subgroups of $G$ a countable set.
Trivially, any subset of the set of $G$ subgroups is countable, and in this research work, we shall base our discussion of a specific type of subgroups, as shall be described in the following definitions. 
\begin{definition}
\label{def:inverse.limit}
Let G be a group, and let $\mathcal{N}:=\{N\trianglelefteq G\}$ the set of all normal subgroups of $G$. Let $I\subset\mathbb{N}$ be a set of indices, for which we shall define the following partial order, $\forall i,j\in I$, $i\leq j$ iff $N_j\subseteq N_i$ iff $\sfrac{G}{N_i}\subseteq\sfrac{G}{N_j}$. So, for each $i\leq j$, there exists an epimorphism $\pi_{ji}:\sfrac{G}{N_
j}\rightarrow\sfrac{G}{N_i}$, which projects $\sfrac{G}{N_
j}$ onto $\sfrac{G}{N_
i}$. Then, $\widehat{G}=\underset{\leftarrow}{lim}\{\sfrac{G}{N_k}\}_{k\in I}:=\{(h_k)_{k\in I}\in\prod_{k\in I}\sfrac{G}{N_k} : \pi_{ji}(h_j)=h_i,\forall i\leq j\}$ is an inverse limit of $\{\sfrac{G}{N_k}\}_{k\in I}$, and is called the \textbf{profinite closure} of $G$.
\end{definition}
\begin{definition}
\label{def:pro.isomorphic}
Let $G$ be any group. a subgroup $H\leq G$ is called \textbf{pro-isomorphic}, if $\widehat{H}\cong\widehat{G}$.
\end{definition}
\begin{definition}
\label{def:zeta.pro.isomorphic}
Let $G$ be any group, and let $\widehat{a_n}(G):=\#\{H\leq G : \widehat{H}\cong\widehat{G}, [G:H]=n\}$, in words, the number of pro-isomorphic subgroups of $G$, of index $n$. The \textbf{pro-isomorphic $\zeta$-function} of $G$ is defined by $\widehat{\zeta_G}(s):=\sum_{n=1}^{\infty}\widehat{a_n}(G)n^{-s}$, for some $s\in\mathbb{C}$. We assume, in this research, that $\widehat{a_n}(G)$ is finite for all $G$. A sufficient condition for this would be that $G$ is finitely-generated, by proposition \ref{prop:finite.number.subgroups}.
\end{definition}
\begin{example}
$G=(\mathbb{Z},+)$. $G$ is an abelian group, and every $H\leq G$ is of the form $H=n\mathbb{Z}=\langle n\rangle$, for some $n\in\mathbb{N}$, which means that $H\cong G$, as both are infinite cyclic groups. From this, it is obvious that $\widehat{H}\cong\widehat{G}$, for every $H\leq G$. Any such $H=n\mathbb{Z}$ is the only subgroup of $G$, which is of index $n$, therefore, the pro-isomoprhic $\zeta$-function of $G$ is $\widehat{\zeta_G}(s)=\sum_{i=1}^{\infty}\widehat{a_n}(G)n^{-s}$, where $\widehat{a_n}(G)=1$, which comes to $\widehat{\zeta_{\mathbb{Z}}}=\sum_{i=1}^{\infty}n^{-s}=\zeta(s)$, the Riemann $\zeta$-function. 
\end{example}
After establishing the basic definitions, we observe a fact that is a major motivation for this research, which says that the Riemann $\zeta$-function decomposes to an infinite product of $\zeta_p$-functions, that is, $\zeta(s)=\prod_p\zeta_p(s)=\prod_p\sum_{k=0}^\infty\frac{1}{p^{ks}}=\prod_p\frac{1}{1-p^{-s}}$, where $p$ is prime, and the product runs over all the prime numbers. Following this fact, regarding the Riemann $zeta$-function, we observe that for any finitely-generated, nilpotent and torsion-free group, $G$, we have the same decomposition as above, for the pro-isomorphic $\zeta$-function, $\widehat{\zeta_G}(s)=\prod_p\widehat{\zeta_{G,p}}(s)$, where $\widehat{\zeta_{G,p}}(s):=\sum_{k=0}^\infty a_{p^{ks}}(G)\frac{1}{p^{ks}}$
\begin{definition}
\label{def.lower.central.series}
Let $G$ be any group, then the \textbf{lower central series} of $G$ is series of subgroups of $G$, defined by the recursive rule, $G_n:=[G,G_{n-1}]$, for every $n\in\mathbb{N}$, where $G_0:=G$.  We recall that $[G,G_n]\leq G$ is the subgroup of commutators, $\{gg_ng^{-1}g_n^{-1} : g\in G,g_n\in G_n\}$
\end{definition}
\begin{definition}
\label{def.nilpotency.class}
Let $G$ be any group. the \textbf{nilpotency class} of $G$ is $min\{n\in\mathbb{N} : G_n=[G,G_{n-1}]=\{e\}\}$, in words, the smallest natural number, such that the subgroup of commutators of the form $[G,G_n]$ is the trivial group. We can extend this definition, and say that the trivial group nilpotency class is $0$.
\end{definition}
\subsection{Linearization}
For finitely-generated torsion-free nilpotent groups we associate nilpotent Lie algebras over $\mathbb{Z}_p$, the ring of $p$-adic integers. We show here the basic properties of $\mathbb{Z}_p$-algebras, as subalgebras of $\mathbb{Q}_p$-algebras, where $\mathbb{Q}_p$ is the fraction field of $\mathbb{Z}_p$.
\begin{proposition}
\label{prop.automorphism.group}
Let $\mathcal{L}_p$ be any $\mathbb{Q}_p$-algebra, with $n=dim\mathcal{L}_p$. Then $G(\mathbb{Q}_p)\leq GL_n(\mathbb{Q}_p)$. This is true for any field $\mathbb{F}$ and $\mathbb{F}$-algebra $\mathcal{L}_{\mathbb{F}}$.
\end{proposition}
\begin{proof}
Choose a basis $B=\{b_1,\dots,b_n\}$ of 
$\mathcal{L}_p$. Let $\varphi\in G(\mathcal{L}_p)$, and $v\in\mathcal{L}_p$. $B$ is a basis, so there are $\lambda_1,\dots,\lambda_n\in\mathbb{Q}_p$, such that $v=\lambda_1b_1,\dots,\lambda_nb_b$. Then, $\varphi(v)=\varphi(\sum_{i=1}^n\lambda_ib_i)=\sum_{i=1}^n\varphi(\lambda_ib_i)=\sum_{i=1}^n\lambda_i\varphi(b_i)$. Mark $B_\varphi=\{\varphi(b_1),\dots,\varphi(b_n)\}$. $B_\varphi$ must span $\mathcal{L}_p$, otherwise, there exists a vector $u\in\mathcal{L}_p$, such that $\sum_{i=1}^n\rho_i\varphi(b_i)\neq u$, for any $\rho_1,\dots,\rho_n\in\mathbb{Q}_p$. But $u=\sum_{i=1}^n\tau_ib_i$, $\varphi$ is an automoprhism, so $u=\varphi\varphi^{-1}(u)=\varphi(\varphi^{-1}(\sum_{i=1}^n\tau_ib_i))=\varphi(\sum_{i=1}^n\varphi^{-1}(\tau_ib_i))=\varphi(\sum_{i=1}^n\tau_i\varphi^{-1}(b_i))=\sum_{i=1}^n\varphi(\tau_i\varphi^{-1}(b_i))=\sum_{i=1}^n\tau_i\varphi(\varphi^{-1}(b_i))=\sum_{i=1}^n\tau_ib_i$, in contradiction to the assumed. $B_\varphi
$ is also linearly-independent, because, supposed that $\sum_{i=1}\rho_i\varphi(bi)=0$, then, since $\varphi$ is an automorphism, must be that $\varphi^{-1}(\sum_{i=1}\rho_i\varphi(bi))=0$, which means that $\sum_{i=1}\varphi^{-1}(\rho_i\varphi(bi))=\sum_{i=1}\rho_i\varphi^{-1}(\varphi(bi))=\sum_{i=1}\rho_ib_i=0$, which contradicts the fact that $B$ is a basis. So, $B_\varphi$ is also a basis of $\mathcal{L}_p$. It is immediate to conclude that the matrix representing $\varphi$ is an inverse $n\times n$ matrix, which means that $\varphi\in GL
_n(\mathbb{Q}_p)$, and that $\forall\varphi,\psi\in G(\mathbb{Q}_p)$, their compositions $\varphi\psi$, and $\psi\varphi$ are also in $G(\mathbb{Q}_p)$, which means that $G(\mathbb{Q}_p)\leq GL_n(\mathbb{Q}_p)$.
\end{proof}
\begin{proposition}
\label{prop.integers.subalgebra.rationals}
Let $p$ be any prime number, then $L_p<\mathcal{L}_p$
\end{proposition}
\begin{proof}
Let $B=\{b_1,\dots,b_n\}$ a basis of $\mathcal{L}_p$, so, for any $v\in\mathcal{L}_p$, we have that $v=\lambda_1b_1+\dots+\lambda_nb_n$, where $r\lambda_1,\dots,\lambda_n\in\mathbb{Q}_p$. Obviously, for any prime number $p$, we have that $\mathbb{Z}_p\subset\mathbb{Q}_p$, in other words, $\iota:\mathbb{Z}_p\rightarrow\mathbb{Q}_p$ is a monomorphism of rings. This means that the ring $\mathbb{Z}_p$ acts on the left $\mathbb{Q}_p$-module $\mathcal{L}_p$ by restriction of scalars, that is, for every $r\in\mathbb{Z}_p$, and $s\in\mathcal{L}_p$, we have that $rs:=\iota(r)s$, which is well defined, because $\iota(r)\in\mathbb{Q}_p$. This means that $\mathcal{L}_p$ inherits the structure of a left $\mathbb{Z}_p$-module. We mark $L_p:=\{r_1b_1,\dots,r_nb_n\}$, where $r1_,
\dots,r_n\in\mathbb{Z}_p$. $B$ is generating $L_p$, by the construction, and it is clear that $B$ is $\mathbb{Z}_p$-linearly-independent, since $B$ is $\mathbb{Q}_p$-linearly-independent, and $\mathbb{Z}_p\subset\mathbb{Q}_p$. So, $B$ is a basis also for $L_p$, and it is clear that any $\mathbb{Z}_p$-linear combination of vectors of $B$ is a $\mathbb{Q}_p$-linear combination of vectors of $B$, hence $L_p\leq\mathcal{L}_p$.
\end{proof}
\begin{proposition}
\label{prop.padic.field.algebra}
Let $p$ be a prime number, and let $L_n(\mathbb{Z}_p)$ be a $\mathbb{Z}_p$-algebra, for any $n\in\mathbb{N}$, then there exists a $\mathbb{Q}_p$-algebra, denoted by $\mathcal{L}_n(\mathbb{Q}_p)$, such that \ref{prop.integers.subalgebra.rationals} holds.
\end{proposition}
\begin{proof}
We construct $\mathcal{L}_n(\mathbb{Q}_p)$ from $L_n(\mathbb{Z}_p)$ by extension of scalars. $\mathbb{Q}_p$ is a $\mathbb{Z}_p$-module, as a trivial fact, so, taking the tensor product $L_n\otimes_{\mathbb{Z}_p}\mathbb{Q}_p$, we have, for all $a\in\L_n(\mathbb{Z}_p)$, and for all $r,r'\in\mathbb{Q}_p$, that $r'(a\otimes r)=a\otimes rr'=rr'(a\otimes 1)$, which is well-defined, because of the multiplication in $\mathbb{Q}_p$, and so, given $B=\{b_1,b2_,\dots,b_n\}$, a basis for $L_n(\mathbb{Z}_p)$, we have a natural bijection between $b_i$ and $b_i\otimes 1$, for $1\leq i\leq n$, which means that $\{b_1\otimes 1,b_2\otimes 1,\dots,b_n\otimes 1\}$ is a basis for $L_n\otimes_{\mathbb{Z}_p} \mathbb{Q}_p$, we denote $\mathcal{L}_n(\mathbb{Q}_p):=L_n\otimes_{\mathbb{Z}_p} \mathbb{Q}_p$, and we got that $\mathcal{L}_n(\mathbb{Q}_p)$ is a $\mathbb{Q}_p$-algebra, with the same basis, $B$, but with scalars from $\mathbb{Q}_p$, which proves that $L_n<\mathcal{L}_n$.
\end{proof}
\begin{proposition}
\label{prop.integer.automorphism}
Let $B=\{b_1,\dots,b_n\}$ be any basis of $\mathcal{L}_p$, and $\varphi\in G(\mathbb{Q}_p)$ any $\mathbb{Q}_p$-automorphism. Then $\varphi(L_p)\subseteq L_p$ iff $\varphi(b_1),\dots,\varphi(b_n)\in L_p$
\end{proposition}
\begin{proof}
Clearly, if $\varphi(v)\in L_p$, for every $v\in L_p$, then also $\varphi(b_1),\dots,\varphi(b_n)\in L_p$. We prove the opposite by taking $v=r_1b_1+\dots+r_nb_n$, then $\varphi(v)=\varphi(\sum_{i=1}^nr_ib_i)=\sum_{i=1}^n\varphi(r_ib_i)=\sum_{i=1}^nr_i\varphi(b_i)$, but $\varphi(b_1),\dots,\varphi(b_n)\in L_p$, so $\sum_{i=1}^nr_i\varphi(b_i)$ is a $\mathbb{Z}_p$-linear combination, hence $\varphi(v)\in L_p$
\end{proof}
\begin{proposition}
\label{prop.monoid}
Let $G^{+}(\mathbb{Q}_p):=G(\mathbb{Q}_p)\cap\mathcal{M}_n(\mathbb{Z}_p)=\{\varphi\in Aut_{\mathbb{Q}_p}(\mathcal{L}_p) : \varphi\in\mathcal{M}_n(\mathbb{Z}_p)\}$, in words, all the $\mathcal{L}_p$-automorphisms, which are matrices over $\mathbb{Z}_p$. Then, $G^{+}(\mathbb{Q}_p)$ is a monoid.
\end{proposition}
\begin{proof}
$G(\mathbb{Q}_p)$ is a group, thus a monoid, and $\mathcal{M}_n(\mathbb{Z}_p)$ is a monoid, so, their intersection is a monoid.
\end{proof}
\begin{proposition}
\label{prop.right.coset}
Let $g\in G^{+}(\mathbb{Q}_p)$, then, the right coset $G(\mathbb{Z}_p)g\subseteq G^{+}(\mathbb{Q}_p)$
\end{proposition}
\begin{proof}
Let $h\in G(\mathbb{Z}_p)$. We proved in \ref{prop.integers.subalgebra.rationals} that $L_p\leq\mathcal{L}_p$, so $h\in G(\mathbb{Q}_p)$. But, $h(L_p)\subseteq L_p$, and from \ref{prop.integer.automorphism}, we know that $h$ is a $\mathbb{Z}_p$-linear combination of vectors in $L_p$, which means that $h$ is a matrix with coefficients in $\mathbb{Z}_p$, that means, $h\in\mathcal{M}_n(\mathbb{Z}_p)$, so $h\in G({\mathbb
{Q}_p})\cap\mathcal{M}_n(\mathbb{Z}_p)$, which means that $hg\in G({\mathbb
{Q}_p})\cap\mathcal{M}_n(\mathbb{Z}_p)$.
\end{proof}
\begin{corollary}
\label{cor.coset.union}
$G^+(\mathbb{Q}_p)=\bigsqcup_{i=1}^n G(\mathbb{Z}_p)g_i$, where $[G(\mathbb{Q}_p):G(\mathbb{Z}_p)]=n$
\end{corollary}
\begin{proposition}
\label{prop.prop.coset.bijection}
There is a bijection between $G(\mathbb{Z}_p)\backslash G^+(\mathbb{Q}_p)$ and $\{M\leq L_p : M\cong L_p\}$
\end{proposition}
\begin{proof}
Let $\varphi\in G(\mathbb{Z}_p)g|\in G(\mathbb{Z}_p)\backslash G^+(\mathbb{Q}_p)$, and let $M=\varphi(L_p)$. But, from \ref{prop.right.coset}, we have that $\varphi\in G^+(\mathbb{Q}_p)$, so $M=\varphi(L_p)\subseteq L_p$. Choose a different representative $\psi\in G(\mathbb{Z}_p)g$, we have that $\tau=\psi\varphi^{-1}\in G(\mathbb{Z}_p)$, which means that $\tau(L_p)=L_p$. But $\tau\varphi=\psi\varphi^{-1}\varphi=\psi$, which means that $\psi(L_p)=\tau\varphi(L_p)=\varphi(\tau(L_p))=\varphi(L_p)=M$, so we have that $M$ is the image of any representative of $G(\mathbb{Z}_p)g$. Let $\varphi|_{L_p}:L_p\rightarrow M$ be the restriction of $\varphi$ to $L_p$. Obviously, $\varphi|_{L_p}$ is onto $M$, and is one-to-one, as a restriction of an automorphism. So we have that $\varphi|_{L_p}$ is an isomorphism, which means that $L_p\cong M$. So, we conclude that every right coset of the form $G(\mathbb{Z}_p)g$ defines an isomomorphism of the form $L_p\cong M$. For the opposite direction, we show that for every isomorphism $L_p\cong M$, we can find some $\varphi\in G^{+}(\mathbb{Q}_p)$, for which $\varphi(L_p)=M$. Choose another automoprhism, $\psi\in G^{+}(\mathbb{Q}_p)$, such that $\psi(L_p)=M$, and let $\tau=\varphi\psi^{-1}$. But $\tau(L_p)=\varphi\psi^{-1}(L_p)=\psi^{-1}(\varphi(L_p))=\psi^{-1}(M)=L_p$, which means that $\tau\in G(\mathbb{Z}_p)$. But $\tau\psi=\varphi\psi^{-1}\psi=\varphi\in G(\mathbb{Z}_p)\psi$, and, obviously, $\tau^{-1}\varphi=\tau^{-1}\tau\psi=\psi$ means that also $\psi\in G(\mathbb{Z}_p)\varphi$, so $\varphi$ andf $\psi$ are in the same right coset of $\mathbb{Z}_p$, so we have that every isomorphism $L_p\cong M$ is common to all representatives of the same right coset of $G(\mathbb{Z}_p)$, proving the bijection.
\end{proof}
\begin{proposition}
\label{prop.index.submodule}
Let $G(\mathbb{Z}_p)g\in G(\mathbb{Z}_p)\backslash G^+(\mathbb{Q}_p)$ be a right coset, and $M\leq L_p$ the image of this coset, as constructed in \ref{prop.prop.coset.bijection}, then $[L_p:M]=|det g|_p^{-1}$.
\end{proposition}
\subsection{$p$-adic Integration}
\begin{proposition}
\label{prop.topological.group.open.set}
Let $\Gamma$ be a topological group, and let $U\subseteq\Gamma$, be an open subset in $\Gamma$. Then $\gamma U:=\{\gamma u : \gamma\in\Gamma, u\in U\}$ is also an open subset of $\Gamma$.
\begin{proof}
We define a map $f=f_{\gamma^{-1}}:\Gamma\rightarrow\Gamma$, by $f(g):=\gamma^{-1}g$, for any $g\in\Gamma$. Clearly, $f$ is continuous, as a composition of continuous maps, that is, the inverse map $\gamma\mapsto\gamma^{-1}$, and the multiplication map $(\gamma^{-1},g)\mapsto\gamma^{-1}g$, so any inverse image $f^{-1}$ of an open subset is an open subset. But $f^{-1}(U)=\{g\in G : f(g)=\gamma^{-1}g\in U\}=\{\gamma h : f(\gamma h)=\gamma^{-1}\gamma h=h\in U\}=\{\gamma h : h\in U\}=\gamma U$, which proves that $\gamma U$ is an open subset in $\Gamma$.
\end{proof}
\end{proposition}
\begin{proposition}
\label{prop.compact.subset.right.haar.measure}
Let $\Gamma$ be a locally compact topological group, i.e., $\forall\gamma\in\Gamma$, there is an open environment $U_{\gamma}$ of $\gamma$, and a compact subset $K_{\gamma}$, such that $\gamma\in U_{\gamma}\subset K_{\gamma}$. Then there is a measure $\mu$, with the following property: for any measurable subset, $U\subseteq\Gamma$, and any $\gamma\in\Gamma$, $\mu(U\gamma)=\mu(U)$, where $U\gamma:=\{u\gamma : u\in U\}$, and $\mu$ is unique up to multiplication in constant. $\mu$ is called a \textbf{Right Haar Measure}
\end{proposition}
\begin{proposition}
\label{prop.padic.field.automorphisms.locally.compact.group}
Let $p$ be a prime number, then $G(\mathbb{Q}_p)$ is a locally compact topological group.
\end{proposition}
\begin{proposition}
\label{prop.padic.field.automorphisms.haar.measure}
Let $p$ be a prime number, then $G(\mathbb{Q}_p)$ has a unique right Haar measure $\mu$, with the following property: $\mu(G(\mathbb{Z}_p))=1$.
\end{proposition}
\begin{corollary}
\label{cor.haar.measure.padic.integers.cosets}
For every $g\in G(\mathbb{Q}_p)$, we have that $\mu(G(\mathbb{Z}_p)g)=\mu(G(\mathbb{Z}_p))=1$
\end{corollary}
\begin{proposition}
\label{prop.padic.integers.integral}
Let $p$ be a prime number, $s\in\mathbb{C}$, and let $g\in G^+(\mathbb{Q}_p)$, then $|det(g)|_p^s=\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}|det(h)|_h^sd\mu$.
\end{proposition}
\begin{proof}
We saw in \ref{prop.index.submodule} that for every $g\in G(\mathbb{Z}_p)h$, we have that $|det(g)|_p^{-1}$ does not depend on the choice of representative, which means that $|det(g)|_p$ is constant on the entire coset, which means that $|det(g)|_p=|det(h)|_p$. so, $|det(g)|_p^s=\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}|det(h)|_p^sd\mu=\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}|det(g)|_p^sd\mu=|det(g)|_p^s\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}d\mu$. But, $\mu=\mu(G(\mathbb{Z}_p)h)$, and we saw in \ref{cor.haar.measure.padic.integers.cosets} that $\mu(G(\mathbb{Z}_p)h)=\mu(G(\mathbb{Z}_p))=1$, so $\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}|det(h)|_p^sd=|det(g)|_p^s\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}d\mu=|det(g)|_p^s\cdot 1=|det(g)|_p^s$
\end{proof}
\begin{corollary}
\label{cor.proisomorphic.integral}
Let $p$ be a prime number, $s\in\mathbb{C}$, then $\widehat{\zeta_{L,p}}(s)=\displaystyle\sum_{G(\mathbb{Z})g\in G(\mathbb{Z}_p)\backslash G^+(\mathbb{Q}_p)}|det(g)|_p^s=\displaystyle\sum_{G(\mathbb{Z})g\in G(\mathbb{Z}_p)\backslash G^+(\mathbb{Q}_p)}\displaystyle\int_{h\in G(\mathbb{Z}_p)g}|det(h)|_p^s d\mu=\displaystyle\int_{h\in G^+(\mathbb{Q}_p)}|det(h)|_p^s d\mu$
\end{corollary}
\begin{theorem}
\label{thm.rational.function}
Let $p$ be a prime number, $s\in\mathbb{C}$, then there exists a rational function, $w_p(s):=\frac{f(x)}{g(x)}$, where $f,g\in\mathbb{Z}_p[x]$, which satisfies $\widehat{\zeta_{L,p}}(s)=w_p(p^{-s})$.
\end{theorem}
\section{Research}
\subsection{The group $U_n$}
\begin{proposition}
\label{prop.integer.matrix.inverse}
Let $A\in\mathcal{M}_n(\mathbb{Z}_p)$, then $A\in GL_n(\mathbb{Z}_p)$ iff $det(A)=\pm 1$
\end{proposition}
\begin{proof}
We shall prove only one direction. Assume $det(A)=\pm 1$. 
For $A$, we calculate the adjoint matrix, $Adj(A)$, by calculating minors for all the elements of $A$, and create the cofactor matrix of $A$, then take the transpose of the cofactor matrix. With this calculation, $A^{-1}=\frac{Adj(A)}{det(A)}$, and, since calculating minors requires multiplication and subtraction between elements of the $A$. But $\mathbb{Z}_p$ is a ring, so closed under multiplication and subtraction, hence all the minors are also in the ring, which means that $Adj(A)\in\mathcal{M}_n(\mathbb{Z}_p)$. But $det(A)=\pm 1$, so $A^{-1}=\frac{Adj(A)}{det(A)}=\pm 1\cdot Adj(A)\in\mathcal{M}_n(\mathbb{Z}_p)$, which means that $A,A^{-1}\in GL_n(\mathbb{Z}_p)$.
\end{proof}
\begin{corollary}    
\label{cor.unipotent.matrix.inverse}
Let $A$ be a $n\times n$ matrix, with $1$ on the main diagonal, and $a_{ij}\in\mathbb{Z}_p$, where $i<j$, and all the other elements, namely, the elements below the main diagonal, are $0$. Then every matrix $A$ of this form has an inverse matrix, $A^{-1}$, of the same form.
\begin{proof}
One checks that all the minors of the elements on the main diagonal are $1$, all the minors of the elements above the main diagonal are $0$, and all the minors of the elements below the main diagonal can be any $p$-adic integers. constructing the cofactor matrix and transposing it, gives a matrix of the form described above, which is the inverse matrix of $A$, as we prove in \ref{prop.integer.matrix.inverse}.
\end{proof}
\end{corollary}
\begin{corollary}
\label{cor.un.group}
Let $U_n$ be the set of all matrices of the form described in \ref{cor.unipotent.matrix.inverse}, then $(U_n, \cdot)$ is a group, where $\cdot$ is the standard matrix multiplication.
\end{corollary}
\begin{proof}
One checks that multiplying two matrices in $U_n$ gives a product matrix of the same form. Associativity comes from the standard matrix multiplication, and clearly, the standard unit matrix, $I_n$, is also in $U_n$. By \ref{cor.unipotent.matrix.inverse}, we know that $A$ has an inverse matrix, $A^{-1}$, which is of the same form, hence $A^{-1}\in U_n$, which completes the proof.
\end{proof}
\begin{proposition}
\label{prop.unipotent.matrix.integer.addition}
Let $E_{ij}\in U_n$, where $1\leq i<j\leq n$, be a $n\times n$ matrix, with $1$ on the main diagonal, and $a_{ij}=1$, and $0$ anywhere else. Then, $E_{ij}^m$ is a matrix of the same form, except that $a_{ij}=m$.
\end{proposition}
\begin{proof}
By induction on $m$. For $m=1$, $E_{ij}^1=E_{ij}$, so, trivially, $a_{ij}=1=m$.
For $m+1$, we have that $E_{ij}^{m+1}=E_{ij}^mE_{ij}$. But $E_{ij}^m$ has that $a_{ij}=m$, by the assumption, and one checks that $E_{ij}^mE_{ij}=\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & m & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & m+1 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}$, which proves the induction step, and the proposition.
\end{proof}
\begin{corollary}
\label{cor.unipotent.matrix.integer.addition}
Let $A=E_{ij}^m$, and $B=E_{ij}^r$. Then, $D=AB=E_{ij}^mE_{ij}^r$ is the matrix with $1$ on the main diagonal, $d_{ij}=m+r$, and all the other elements are $0$.
\end{corollary}
\begin{proof}
One checks that the product matrix has also $1$ on the main digonal, and all the other elements are $0$, except for $d_{ij}=1\cdot r+m\cdot 1=m+r$
\end{proof}
\begin{proposition}
\label{prop.unipotent.matrix.integer.inverse}
Let $E_{ij}\in U_n$ be a matrix of the form described in \ref{prop.unipotent.matrix.integer.addition}. then $E_{ij}^{-m}$ is a matrix of the same form, but with $a_{ij}=-m$.
\end{proposition}
\begin{proof}
From \ref{prop.unipotent.matrix.integer.addition}, we have that $E_{ij}^m=\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & m & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}$. 
From \ref{cor.unipotent.matrix.inverse}, we know that $B=(E_{ij}^m)^{-1}\in U_n$, that is, $B=\begin{pmatrix}
1 & b_{12} & \dots & \dots & b_{1n}\\
0 & 1 & b_{23} & \dots & b_{2n}\\
0 & 0 & 1 & bij & b_{in}\\
0 & 0 & 0 & 1 & b_{n-1n}\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}$, which means that
$\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & m & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}\begin{pmatrix}
1 & b_{12} & \dots & \dots & b_{1n}\\
0 & 1 & b_{23} & \dots & b_{2n}\\
0 & 0 & 1 & b_{ij} & b_{in}\\
0 & 0 & 0 & 1 & b_{n-1n}\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}$. Easy to check that all the elements $b_{kl}$ must be $0$, but for the element in row $i$ and column $j$, we must have that $1\cdot b_{ij}+m\cdot 1=0$, which means that $b_{ij}=-m$. One checks that the same calculation holds for $(E_{ij}^{-1})^m$, and so, $E_{ij}^{-m}=(E_{ij}^m)^{-1}=(E_{ij}^{-1})^m$ is of the form described above.
\end{proof}
\begin{proposition}
\label{prop.commutators}
Let $A=E_{ij},B=E_{kl}\in U_n$ be two matrices of the form described in \ref{cor.unipotent.matrix.inverse}, i.e., $1$ on the main diagonal, and all the other elements are $0$, except for $a_{ij}$ and $b_{kl}$, which are $1$. Then the commutator, $[E_{ij},E_{kl}] =
\left\{
	\begin{array}{ll}
		E_{il}, & j=k\\
        E_{kj}^{-1}, & i=l\\
        0, & j\neq k\land i\neq l
	\end{array}
\right.$
\end{proposition}
\begin{proof}
There are two cases, The first, is where $j=i+1$, or $l=k+1$, and the second is where $j>i+1$, and $l>k+1$. One checks that the proposition is true in both cases.
\end{proof}
\begin{corollary}
\label{cor.unipotent.generators}
Let $A=E_i=E_{ii+1}\in U_n$, be a matrix of the form described in \ref{prop.unipotent.matrix.integer.addition}, where the only $1$ which is outside the main diagonal is one of the elements $a_{12},a_{23},\dots,a_{n-1n}$, i.e., on the diagonal above the main diagonal. Then the set $\mathcal{E}_n=\{E_1,E_2,\dots,E_{n-1}\}$ is a set of generators for the unipotent group $U_n$.
\begin{proof}
By proposition \ref{prop.commutators}, we can create any matrix $E_{ij}\in U_n$ by composition of commutators of the form $[E_i,[E_{i+1},[\dots[E_{j-2},E_{j-1}]]]$. From \ref{prop.unipotent.matrix.integer.addition}, we know that $A=E_{ij}^m$ has that $a_{ij}=m\in\mathbb{Z}_p$, and by \ref{cor.unipotent.matrix.integer.addition}, we know that $D=AB=E_{ij}^mE_{ij}^r$ is the matrix with $1$ on the main diagonal, and all the other elements are $0$, except for $d_{ij}=m+r$. Checking further gives that if $A=E_{ij}^m$ and $B=E_{jk}^r$, then the commutator $[E_{ij^m},E_{kl}^r]$ is the matrix with $1$ on the main diagonal, and all the other elements are $0$, except for $d_{ik}=mr$. Easy to see how to apply the above calculations also for the inverse matrices. This means that we can generate any matrix in $U_n$, by multiplying matrices that come from commutators on the set $\mathcal{E}_n=\{E_1,E_2,\dots,E_n-1\}$, which means that $\mathcal{E}_n$ generates the unipotent group $U_n$.
\end{proof}
\end{corollary}
\begin{proposition}
\label{prop.unipotent.group.nilpotent}
The unipotent group $U_n$ is nilpotent, of nilpotency class $n$.
\end{proposition}
\begin{proof}
Easy to observe that for the set of generators, $\mathcal{E}_n$, the longest composition of commutators, $[E_{i_1},[E_{i_2},[\dots[E_{i_k}]]]$ has that $i_1=1,i_2=2,\dots,i_k=n-1$, or $i_1=n-1,i_2=n-2,\dots,i_k=1$, which means that composing $n-1$ commutators of elements in $\mathcal{E}_n$ leaves only $E_{1n-1}$ and $E_{1n-1}^{-1}$, but $[E_{1n-1},E_{1n-1}^{-1}]=[E_{1n-1}^{-1},E_{an-1}]=I_n$. One can check that this holds for the unipotent group $U_n$ itself, as generated by $\mathcal{E}_n$.
\end{proof}
\begin{proposition}
\label{prop.unipotent.group.torsion.free}
The unipotent group $U_n$ is torsion free.
\end{proposition}
\begin{proof}
Again, we show the proposition for the set of generators, $\mathcal{E}_n$. Let $A=E_i\in\mathcal{E}_n$, and suppose it has a finite order, which means that there exists a $m\in\mathbb{N}$, such that $E_i^m=I_n$. But by $\ref{prop.unipotent.matrix.integer.addition}$, we know that $E_i^m$ is the matrix with $a_{ii+1}=m$, which means that $m=0$, which is a contradiction.
\end{proof}
\subsection{The algebra $L_n$}
\begin{proposition}
\label{prop.lie.algebra.commutator}
Let $E_{ij}\in\mathcal{E}_n$. Let $e_{ij}=E_{ij}-I_n$, in words, $e_{ij}$ is obtained by replacing all the $1$ on the main diagonal with $0$. Then $e_{ij}e_{jk}=e_{ik}$, and $e_{jk}e_{ij}=0_n$, where $0_n$ is the $n\times n$ zero matrix.
\end{proposition}
\begin{proof}
Clearly, since $A=e_{ij}$ has a single non-zero element, $a_{ij}=1$, and $B=e_{jk}$ has that $b_{jk}=1$, then the product matrix $D=AB=e_{ij}e_{jk}$ has a single element, $d_{ik}=a_{ij}b_{jk}=1\cdot 1=1$, but in the product $BA=e_{jk}e_{ij}$, we observe that $b_{jk}$ is multiplied by all the elements of the $k$th row of $A$, which is all zeros, and $a_{ij}$ is being multiplied by the $i$th column of $B$, which is all zeros, thus the product matrix, $BA$, is all zeros.
\end{proof}
\begin{corollary}
\label{cor.lie.algebra}
Let $\mathcal{B}_n$ be the set $\{e_{ij} : i<j\}$, of all the matrices of the form described in \ref{prop.lie.algebra.commutator}. Then $\mathcal{B}_n$, with the standard matrix addition, and a multiplication operation $\ast$, defined by $e_{ij}*e_{jk}=e_{ij}e_{jk}-e_{jk}e_{ij}$, is a basis for a Lie algerba over $\mathbb{Z}_p$, which shall be denoted by $L_n(\mathbb{Z}_p)$, or $L_n$, for abbreviation, which is the $\mathbb{Z}_p$-algebra of all the matrices $A\in\mathcal{M}_n(\mathbb{Z}_p)$, with $0$ on the main diagonal. The multiplication operation $\ast$ shall be denoted by Lie Brackets, that is, $e_{ij}*e_{jk}=[e_{ij},e_{jk}]$.
\end{corollary}
\begin{proof}
Since we have defined the multiplication operation as the standard Lie brackets, for matrix Lie algebras, i.e., $[A,B]=AB-BA$, for all the matrices $A,B$ in the algebra, one easily checks that all the axioms of a Lie algebra hold for this definition. Obviously, $L_n$ is a $\mathbb{Z}_p$-span of $\mathcal{B}_n$, since every matrix of the form $A=\begin{pmatrix}
0 & a_{12} & a_{13} & \dots & a_{1n}\\
0 & 0 & a_{23} & \dots & a_{2n}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \dots & a_{n-1n}\\
0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$ is a linear combination of matrices of $\mathcal{B}_n$, i.e., $A=\sum_{i=1}^{n-1}\sum_{j=i+1}^n a_{ij}e_{ij}$. We can observe that if $B=\sum_{i=1}^{n-1}\sum_{j=i+1}^n b_{ij}e_{ij}=0_n$, then clearly all the $b_{ij}$ are $0$. We conclude that $\mathcal{B}_n$ is a basis for $L_n$.
\end{proof}
\begin{proposition}
\label{prop.lie.algebra.dimension}
Let $p$ be a prime number, and let $n\in\mathbb{N}$ be any natural number, then dim $L_n(\mathbb{Z}_p)=\binom{n}{2}$
\end{proposition}
\begin{proof}
From \ref{cor.lie.algebra}, we have that a basis for $L_n(\mathbb{Z}_p)$ is the set of all $e_{ij}$, where $i<j$. For each row $1\leq i\leq n-1$, we have $n-i$ elements of the form $e_{ij}$, which gives, in total, $\frac{n(n-1)}{2}=\frac{n!}{2!(n-2)!}=\binom{n}{2}$ elements of the the basis.
\end{proof}
\begin{proposition}
\label{prop.nilpotent.lie.algebra}
Let $p$ be a prime number, and let $n\in\mathbb{N}$ be any natural number, then $L_n(\mathbb{Z}_p)$ is a nilpotent Lie algebra.
\end{proposition}
\begin{proof}
It is followed directly from \ref{prop.lie.algebra.commutator}, and from \ref{prop.unipotent.group.nilpotent}, since $[e_{ij},e_{jk}]=[E_{ij},E_{jk}]-I_n=E_{ik}-I
_n=e_{ik}$, where the first brackets are Lie Brackets of $L_n(\mathbb{Z}_p)$, and the second brackets are a group commutator of $U_n(\mathbb{Z}_p)$.
\end{proof}
By considering the behavior of $\mathcal{L}_n(\mathbb{Q}_p)$ under the Lie brackets, we can learn about the structure of $Aut_{\mathbb{Q}_p}(\mathcal{L}_n)$. As a basic fact, every $\mathcal{L}_n(\mathbb{Q}_p)$-automorphism $\varphi$ must obey the $\mathcal{L}_n$ Lie brackets, meaning that for all $x,y\in\mathcal{L}_n$, we must have that $\varphi([x,y])=[\varphi(x),\varphi(y)]$. Let $B=\{b_1,b_2,\dots,b_m\}$ be a basis for $\mathcal{L}_n$, we have that $x=\sum_{i=1}^m\lambda_i b_i$, and $y=\sum_{i=1}^m\rho_i b_i$, so $\varphi([x,y])=[\varphi(\sum_{i=1}^m\lambda_i b_i),\varphi(\sum_{i=1}^m\rho_i b_i)]=[\sum_{i=1}^m\varphi(\lambda_i b_i),\sum_{i=1}^m\varphi(\rho_i b_i)]=[\sum_{i=1}^m\lambda_i\varphi(b_i),\sum_{i=1}^m\rho_i\varphi(b_i)]=\sum_{i=1}^m\sum_{j=1}^m[\lambda_i\varphi(b_i),\rho_j\varphi(b_j)]=\sum_{i=1}^m\sum_{j=1}^m\lambda_i\rho_j[\varphi(b_i),\varphi(b_j)]$. This technique can be demonstrated in the most simple case, which is the Heisenberg group.
\subsection{The Heisenberg group}
\begin{definition}
\label{def.heisenberg.group}
The \textbf{Heisenberg group} is the unipotent group of $3\times 3$ matrices, over $\mathbb{Q}_p$, namely $U_3(\mathbb{Q}_p)$. Every matrix $A\in U_3$ is of the form $$
\begin{pmatrix}
1 & a_{12} & a_{13}\\
0 & 1 & a_{23}\\
0 & 0 & 1\\
\end{pmatrix}
$$ where $a_1,a_2,a_3\in\mathbb{Q}_p$.
\end{definition}
The $\mathbb{Q}_p$-algebra associated with $U_3$ consists of matrices of the form $$A-I_3=
\begin{pmatrix}
0 & a_{12} & a_{13}\\
0 & 0 & a_{23}\\
0 & 0 & 0\\
\end{pmatrix}=a_{12}e_{12}+a_{13}e_{13}+a_{23}e_{23}
$$. Let $\varphi\in Aut_{\mathbb{Q}_p}(\mathcal{L}_p)$ be an $\mathcal{L}_{p,n}$-automorphism. The image of every $A\in\mathcal{L}_{p,n}$, as a linear combination of elements of the basis, is a linear combination of the images of these elements. So, let $v=(x,y,z)=xe_{12}+y_{23}+z_{13}$, where $x,y,z\in\mathbb{Q}_p$, we have that $\varphi(v)=\begin{pmatrix}
x & y & z\\
\end{pmatrix}\begin{pmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{pmatrix}=\begin{pmatrix}
a_{11}x+a_{12}x+a_{13}x & a_{21}y+a_{22}y+a_{23}y & a_{31}z+a_{32}z+a_{33}z\\
\end{pmatrix}=\\\begin{pmatrix}
(a_{11}+a_{12}+a_{13})x & (a_{21}+a_{22}+a_{23})y & (a_{31}+a_{32}+a_{33})z\\
\end{pmatrix}=\begin{pmatrix}
\varphi(x) & \varphi(y) & \varphi(z)\\
\end{pmatrix}
$, which means that $$
\varphi(e_{12})=a_{11}e_{12}+a_{12}e_{23}+a_{13}e_{13}$$
$$\varphi(e_{23})=a_{21}e_{12}+a_{22}e_{23}+a_{23}e_{13}$$
$$\varphi(e_{13})=a_{31}e_{12}+a_{32}e_{23}+a_{33}e_{13}
$$.
We want to find relations between the elements of $\varphi$. Considering the fact that $[\varphi(x),\varphi(y)]=\varphi([x,y])=$, we observe that the Lie brackets on images of any two commuting elements of the basis give $0$, as they are images of $0$, i.e., for every $x,y\in\mathcal{L}_n$, such that $[x,y]=0$, we have that $[\varphi(x),\varphi(y)]=\varphi([x,y])=\varphi(0)=0$. Hence, the only images that do not vanish under Lie brackets are $[\varphi(e_{12}),\varphi(e_{23})]=[a_{11}e_{12}+a_{12}e_{23}+a_{13}e_{13},a_{21}e_{12}+a_{22}e_{23}+a_{23}e_{13}]=a_{11}a_{21}[e_{12},e_{12}]+a_{11}a_{22}[e_{12},e_{23}]+\dots+a_{13}a_{23}[e_{13},e_{13}]=\varphi([e_{12},e_{23}])=\varphi(e_{13})=a_{31}e_{12}+a_{32}e_{23}+a_{33}e_{13}$. Considering again only the non-vanishing Lie brackets, we have that $[\varphi(e_{12}),\varphi(e_{23})]=a_{11}a_{22}[e_{12},e_{23}]+a_{12}a_{21}[e_{23},e_{12}]=a_{11}a_{22}e_{13}-a_{12}a_{21}e_{13}=(a_{11}a_{22}-a_{12}a_{21})e_{13}=a_{31}e_{12}+a_{32}e_{23}+a_{33}e_{13}=\varphi(e_{13})$. Comparing the scalars, for the three elements of the basis, gives the following relations, $$
a_{31}=0$$
$$a_{32}=0$$
$$a_{33}=(a_{11}a_{22}-a_{12}a_{21})\neq 0
$$
which gives the following matrix, $$\varphi(v)=\begin{pmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
0 & 0 & det(A)\\
\end{pmatrix}
$$
where $A$ is the minor $$
A=\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
\end{pmatrix}
$$
We can observe that for every $v\in\mathcal{L}_{p,n}$, writing $M=\varphi(v)$ lines in the following way,
$$
M=\begin{pmatrix}
\varphi(e_{12})\\
\varphi(e_{23})\\
\varphi(e_{n-1n})\\
\varphi(e_{13})\\
\vdots\\
\varphi(e_{n-1n})\\
\vdots\\
\varphi(e_{1n})\\
\end{pmatrix}\\
$$
where $m=\binom{n}{2}$, divides $M$ to a block matrix, $$M=\begin{pmatrix}
M_{11} & \vline & M_{12}&\vline & \dots& \vline & M_{1n-1} & \vline&M_{1n}\\
\hline
M_{21} & \vline & M_{22}&\vline & \dots &\vline & M_{2n-1} &\vline& M_{2n}\\
\hline
\vdots & \vline & \vdots&\vline & \dots &\vline & \vdots &\vline& \vdots\\
\hline
M_{n1} & \vline & M_{n2}&\vline & \dots &\vline & M_{nn-1} &\vline& M_{nn}\\
\end{pmatrix}
$$
where $M_{ij}\in\mathcal{M}_{k\times l}(\mathbb{Q}_p)$, $k=$dim$(\gamma_i\mathcal{L}),l=$dim$(\gamma_j\mathcal{L})$. From this, we can understand that the blocks on the main diagonal of $M$ are squared matrices, $A_{ii}\in\mathcal{M}_{n-i}$. From the calculation on $\mathcal{L}_{p,3}$, we understand also that any element $e_{ii+k}\in\gamma_k\mathcal{L}_{p,n}$ must vanish in the images of elements from higher nilpotency classes, i.e. $\varphi(e_{i,i+l})$, where $l>k$, which means that all the elements under every squared block on the main diagonal must be zero, so $M$ has the form, $$M=\begin{pmatrix}
M_{11} & \vline & M_{12}&\vline & M_{13} & \dots& \vline & M_{1m-1} & \vline&M_{1m}\\
\hline
0 & \vline & M_{22}&\vline & M_{23} & \dots &\vline & M_{2m-1} &\vline& M_{2m}\\
\hline
\vdots & \vline & \vdots&\vline & \vdots & \dots &\vline & \vdots &\vline& \vdots\\
\hline
0 & \vline & 0&\vline & 0 & \dots &\vline & M_{2m-1} &\vline& M_{2m}\\
\hline
0 & \vline & 0 &\vline & 0 & \dots &\vline & 0 &\vline& M_{mm}\\
\end{pmatrix}
$$
We observe that the matrix $M_{ij}$ blocks represent quotients of the form $\sfrac{\gamma_i\mathcal{L}_{p,n}}{\gamma_{i+1}\mathcal{L}_{p,n}},\sfrac{\gamma_j\mathcal{L}_{p,n}}{\gamma_{j+1}\mathcal{L}_{p,n}}$. We shall state, as a fact, that the block $M_{11}$ is either diagonal or anti-diagonal, i.e., $$M_{11}=\begin{pmatrix}
\lambda_1 & & &\\
& \lambda_2 & &\\
& & \ddots &\\
& & & \lambda_n\\
\end{pmatrix}$$
or
$$M_{11}=\begin{pmatrix}
& & & \lambda_1\\
& & \lambda_2 &\\
& \iddots & &\\
\lambda_n & & &\\
\end{pmatrix}$$
In the case of an anti-diagonal block, we have the following proposition,
\begin{proposition}
\label{prop.involution}
Let $p$ be a prime number, and let $n\in\mathbb{N}$. $B_n=\{e_1,\dots,e_{m-1}\}$, where $m=\binom{n}{2}$. Then, the map $\eta_n:B_n\rightarrow B_n$, defined by $\eta_n(e_i):=e_{m-i}$ is a $\mathcal{L}_{p,n}$-automorphism, which is also an involution.
\end{proposition}
\begin{proof}
Clearly, $\eta_n$ is the anti-diagonal $m\times m$ matrix,$$
\eta_n=\begin{pmatrix}
& & & 1 &\\
& & 1 &\\
& \iddots & &\\
1 & & &\\
\end{pmatrix}
$$
$\eta_n$ is an invertible matrix, which operates on any vector $$v=(a_1,a_2,\dots,a_{m-1})=\sum_{i=1}^{m-1}a_ie_i$$ in the following way,$$
\eta_n(v)=\eta_n\bigg(\sum_{i=1}^{m-1}a_ie_i\bigg)=\begin{pmatrix}
a_1 & a_2 & \dots & a_{m-1}\\
\end{pmatrix}\begin{pmatrix}
& & & 1 &\\
& & 1 &\\
& \iddots & &\\
1 & & &\\
\end{pmatrix}=\begin{pmatrix}
a_{m-1} & a_{m-2} & \dots & a_1\\
\end{pmatrix}=$$
$$\begin{pmatrix}
\eta_n(a_1) & \eta_n(a_2) & \dots & \eta_n(a_{m-1})\\
\end{pmatrix}=\sum_{i=1}^{m-1}\eta_n(a_ie_i)=\sum_{i=1}^{m-1}a_i\eta_n(e_i)
$$
And, $\eta_n^2(v)=\eta_n(\eta_n(v))=\eta_n\bigg(\eta_n\bigg(\sum_{i=1}^{m-1}a_ie_i\bigg)\bigg)=\eta_n\bigg(\sum_{i=1}^{m-1}a_i\eta_n(e_i)\bigg)=\sum_{i=1}^{m-1}a_i\eta_n^2(e_i)=\sum_{i=1}^{m-1}a_i\eta_n(e_{n-i})=\sum_{i=1}^{m-1}a_ie_i$
\end{proof}
From this proposition, we realize that if $M_{11}$ is anti-diagonal, then $\eta_n\varphi$ is the automorphism which has that $M_{11}$ is diagonal.
\begin{proposition}
\label{prop.main.diagonal.blocks}
Let $p$ be a prime number, and let $n\in\mathbb{N}$, and let $M=\varphi\in\mathcal{L}_{p,n}$. Then, all the blocks on the main diagonal, $M_{ii},\dots,M_{n-1n-1}$, are diagonal, of the form,$$
M=\varphi=\begin{pmatrix}
\lambda_1 & & & & & & & &\\
& \lambda_2 & & & & & & &\\
& & \ddots & & & & & &\\
& & & \lambda_n & & & & &\\
& & & & \lambda_1\lambda_2 & & & &\\
& & & & & \lambda_2\lambda_3 & & &\\
& & & & & & \ddots & &\\
& & & & & & & \lambda_{n-1}\lambda_n & &\\
& & & & & & & & \ddots &\\
& & & & & & & & & \lambda_1\lambda_2\cdots\lambda_n\\
\end{pmatrix}
$$
\end{proposition}
\begin{proof}
By simple induction. We have already assumed that $M_{11}$ is diagonal. Every sequential block $M_{ii}$ contains the coefficients of elements of $\sfrac{\gamma_i\mathcal{L}_{p,n}}{\gamma_{i+1}\mathcal{L}_{p,n}}$ as summands in images of elements of the same quotient algebra. So, $\varphi(e_{ii+2})=\sum_{i=1}^{n-2}a_{ii+2}e_{ii+2}$, but $e_{ii+2}=[e_{ii+1},e_{i+1i+2}]$, so $\varphi(e_{ii+2})=[\varphi(e_{ii+1}),\varphi(e_{i+1i+2})]$, hence, $\lambda_{i+2}=a_{ii+2}=a_{ii+1}a_{i+1i+2}=\lambda_{i}\lambda_{i+1}$, which proves the proposition.
\end{proof}
\begin{proposition}
\label{prop.h.matrix.determinant}
Let $n\in\mathbb{N}$, and let $$
A_n=\begin{pmatrix}
\lambda_1 & & & & & & & &\\
& \lambda_2 & & & & & & &\\
& & \ddots & & & & & &\\
& & & \lambda_n & & & & &\\
& & & & \lambda_1\lambda_2 & & & &\\
& & & & & \lambda_2\lambda_3 & & &\\
& & & & & & \ddots & &\\
& & & & & & & \lambda_{n-1}\lambda_n & &\\
& & & & & & & & \ddots &\\
& & & & & & & & & \lambda_1\lambda_2\cdots\lambda_n\\
\end{pmatrix}
$$
where $\lambda_1,\lambda_2,\dots,\lambda_n\in\mathbb{Q}_p$, then, $det(A_n)=\prod_{i=1}^{n}\lambda_i^{i(n+1-i)}$.
\end{proposition}
\begin{proof}
We observe that the determinants, for $n=1,2,3,\dots$, form a recursive sequence, $$
det(A_1)=\lambda_1$$
$$det(A_2)=det(A_1)\lambda_1\lambda_2^2$$
$$det(A_3)=det(A_2)\lambda_1\lambda_2^2\lambda_3^3$$
$$\vdots$$
$$det(A_n)=det(A_{n-1})\lambda_1\lambda_2^2\lambda_3^3\cdots\lambda_n^n$$
Calculating the general element, $a_n=det(A_n)$, we see that we have $n$ times $\lambda_1$, $n-1$ times $\lambda_2^2$, $n-2$ times $\lambda_3^3$, and so forth. In general, we have $n-i+1$ times $\lambda_i^i$, which means that we have $i(n-i+1)$ times $\lambda_i$, and in total, $a_n=det(A_n)=\prod_{i=1}^n\lambda_i^{i(n+1-i)}$.
\end{proof}
This means that every $M=\varphi\in\mathcal{L}_{p,n}$ is of the form,$$M=\varphi=\begin{pmatrix}
\begin{matrix}\lambda_1 & & &\\
& \lambda_2 & &\\
& & \ddots &\\
& & & \lambda_n\\
\end{matrix} & \vline & M_{12}&\vline & M_{13} & \dots& \vline & M_{1m-1} & \vline&M_{1m}\\
\hline
0 & \vline & \begin{matrix}\lambda_1\lambda_2 & & &\\
& \lambda_2\lambda_3 & &\\
& & \ddots &\\
& & & \lambda_{n-1}\lambda_n\\
\end{matrix}&\vline & M_{23} & \dots &\vline & M_{2m-1} &\vline& M_{2m}\\
\hline
\vdots & \vline & \vdots&\vline & \vdots & \dots &\vline & \vdots &\vline& \vdots\\
\hline
0 & \vline & 0&\vline & 0 & \dots &\vline & M_{2m-1} &\vline& M_{2m}\\
\hline
0 & \vline & 0 &\vline & 0 & \dots &\vline & 0 &\vline& \lambda1\lambda2\cdots\lambda_n\\
\end{pmatrix}$$
The above discussion gives rise to the decomposition of each $\varphi\in\mathcal{L}_{p,n}$ to two matrices, one is the diagonal matrix $$h=\begin{pmatrix}
\lambda_1 & & & & & & & &\\
& \lambda_2 & & & & & & &\\
& & \ddots & & & & & &\\
& & & \lambda_n & & & & &\\
& & & & \lambda_1\lambda_2 & & & &\\
& & & & & \lambda_2\lambda_3 & & &\\
& & & & & & \ddots & &\\
& & & & & & & \lambda_{n-1}\lambda_n & &\\
& & & & & & & & \ddots &\\
& & & & & & & & & \lambda_1\lambda_2\cdots\lambda_n\\
\end{pmatrix}
$$
and the other matrix is $$n=\begin{pmatrix}
1 &* &* &* &* &* &* & *\\
& 1 &* &* &* &* &* &*\\
& & \ddots & *& *& *& *& *&\\
& & & 1 &* &* & *& *\\
& & & & 1 &* &* & *\\
& & & & & 1 & *& *\\
& & & & & & \ddots & *\\
& & & & & & & 1
\end{pmatrix}
$$
So, we have the following proposition,
\begin{proposition}
\label{prop.automorphism.matrix,decomposition}
Let $p$ be a prime number, and let $n\in\mathbb{N}$, and let $M=\varphi\in\mathcal{L}_{p,n}$. Then, $M=\varphi=nh$, where $n$ and $h$ are of the above form.
\end{proposition}
\begin{proof}
Trivially, $h$ is an invertible matrix, and its inverse is the matrix $$h^{-1}=\begin{pmatrix}
\lambda_1^{-1} & & & & & & & &\\
& \lambda_2^{-1} & & & & & & &\\
& & \ddots & & & & & &\\
& & & \lambda_n^{-1} & & & & &\\
& & & & (\lambda_1\lambda_2)^{-1} & & & &\\
& & & & & (\lambda_2\lambda_3)^{-1} & & &\\
& & & & & & \ddots & &\\
& & & & & & & (\lambda_{n-1}\lambda_n)^{-1} & &\\
& & & & & & & & \ddots &\\
& & & & & & & & & (\lambda_1\lambda_2\cdots\lambda_n)^{-1}\\
\end{pmatrix}
$$
Easy to check that $n=Mh^{-1}$ is also an invertible matrix, with $1$ on the main diagonal, and $0$ below it.
\end{proof}
We observe that all the matrices with non-zero elements on the main diagonal, and $0$ everywhere else form an abelian subgroup of $G_n(\mathbb{Q}_p)$, since multiplying such matrices yields a matrix of the same specification. Let $$h_{\alpha}=\begin{pmatrix}
\alpha_1 & & &\\
& \alpha_2 & &\\
& & \ddots &\\
& & & \alpha_m\\
\end{pmatrix},h_{\beta}=\begin{pmatrix}
\beta_1 & & &\\
& \beta_2 & &\\
& & \ddots &\\
& & & \beta_m\\
\end{pmatrix}$$
Then, $$h_{\alpha}h_{\beta}=\begin{pmatrix}
\alpha_1\beta_1 & & &\\
& \alpha_2\beta_2 & &\\
& & \ddots &\\
& & & \alpha_m\beta_m\\
\end{pmatrix}=\begin{pmatrix}
\beta_1\alpha_1 & & &\\
& \beta_2\alpha_2 & &\\
& & \ddots &\\
& & & \beta_m\alpha_m\\
\end{pmatrix}=h_{\beta}h_{\alpha}
$$
Obviously, this subgroup, which we shall denote as $H<G_n(\mathbb{Q}_p)$ is not normal, as we observe by taking the $n$ matrix described above, and multiplying $A=nhn^{-1}$, clearly $A\notin H$. On the other hand, the set of all $n$ matrices if a normal subgroup of $G_n(\mathbb{Q}_p)$, because if $$n_{\alpha}=\begin{pmatrix}
1 & \alpha_{12} & \alpha_{13} & \dots & \alpha_{1m}\\
& 1 & \alpha_{23} & \dots & \alpha_{2m}\\
& & \ddots &  \vdots& \vdots &\\
& &  &  1& \alpha_{m-1m} &\\
& & & & 1\\
\end{pmatrix},n_{\beta}=\begin{pmatrix}
1 & \beta_{12} & \beta_{13} & \dots & \beta_{1m}\\
& 1 & \beta_{23} & \dots & \beta_{2m}\\
& & \ddots &  \vdots& \vdots &\\
& &  &  1& \beta_{m-1m} &\\
& & & & 1\\
\end{pmatrix}
$$
Then $$n_\alpha n_\beta=\begin{pmatrix}
1 & \alpha_{12}+\beta_{12} & * & \dots & *\\
& 1 & * & \dots & *\\
& & \ddots &  \vdots& \vdots &\\
& &  &  1& \alpha_{m-1m}+\beta_{m-1m} &\\
& & & & 1\\
\end{pmatrix}
$$
which proves that all the $n$ matrices form a subgroup, which we shall denote by $N\in G_n(\mathbb{Q}_p)$.
taking any matrix, $g\in G_n(\mathbb{Q}_p)$, and taking the product $A=gng^{-1}$, if we look at the main diagonals, we see that the product is of the general form $$
gng^{-1}=\begin{pmatrix}
\lambda_1 & a_{12} & \dots & a_{1m}\\
0 & \lambda_2 & \dots & a_{2m}\\
& & \ddots &*\\
& & & \lambda_m\\
\end{pmatrix}\begin{pmatrix}
1 & b_{12} & \dots & b_{1m}\\
0 & 1 & \dots & b_{2m}\\
& & \ddots &*\\
& & & 1\\
\end{pmatrix}\begin{pmatrix}
\lambda_1^{-1} & c_{12} & \dots & c_{1m}\\
0 & \lambda_2^{-1} & \dots & c_{2m}\\
& & \ddots &*\\
& & & \lambda_m^{-1}\\
\end{pmatrix}=$$
$$
\begin{pmatrix}
1 & d_{12} & \dots & d_{1m}\\
0 & 1 & \dots & d_{2m}\\
& & \ddots &*\\
& & & 1\\
\end{pmatrix}\in N
$$
So, $N\lhd G_n(\mathbb{Q}_p)$ is a normal subgroup.
This discussion gives rise to the decomposition of $G_n(\mathbb{Q}_p)$. 
Since only $N$ is a normal subgroup of $G_n(\mathbb{Q}_p)$, we decompose $G_n(\mathbb{Q}_p)$ to a semi-direct product, $G\cong N\rtimes H$, where the map $\phi:H\rightarrow Aut(N)$, given by $\phi(h)(n):=hnh^{-1}$, for every $h\in H$, and $n\in N$, is a homomorphism, as we can see by the fact that for every $h_1,h_2\in H$, and for every $n\in N$, $\phi(h_1)\phi(h_2)(n)=h_1n(h_2nh_2^{-1})h_1^{-1}=h_1h_2nh_2^{-1}h_1^{-1}=(h_1h_2)n(h_1h_2)^{-1}=\phi(h1h2)
(n)$
This means that calculating the integral, for $G_n(\mathbb{Q}_p)$, reduces to calculating a double integral, $\displaystyle\int_{N\rtimes H}$. We mean to show in the research that the normal subgroup $N$ can itself be decomposed to a semi-direct product of several subgroups, thus simplyifing the integration.



By \ref{prop.integer.automorphism}, we have that any $L_p(\mathbb{Z}_p)$-automorphism must be in $G_n(\mathbb{Z}_p)$, in words, any $\varphi\in G(\mathbb{Z}_p)$ is an invertible matrix with elements in $\mathbb{Z}_p$. Our goal is to find a way to compute $G(\mathbb{Z}_p)$, the automorphism group of $L_n(\mathbb{Z}_p)$, for any $n\in\mathbb{N}$. After finding a general formula for this calculation, we shall be able to show a way to compute the $n$-multiple $p$-adic integral of the form $\displaystyle\int\int\dots\int\int_{D_1\times D_2\dots\times D_{n-1}\times D_n}f(h_1,h_2,\dots,h_{n-1},h_n)d(\mu_1,\mu_2,\dots,\mu_{n-1},\mu_n)$, where $D_i$ is the set of $G(\mathbb{Z}_p)$-cosets, for $G(\mathbb{Z}_p)$, the group of $\mathbb{Z}_p$-automorphisms on the algebra $L_i(\mathbb{Z}_p)$, and $h_i$ is any element of this group, and $\mu_i$ is the Haar measure on this group. By Fubini, this multiple integral can be calculated as the iterated integral $$\displaystyle\int_{D_n}\left(\int_{D_{n-1}}\dots\left(\int_{D_2}\left(\int_{D_1}f(h_1,h_2,\dots,h_{n-1},h_n)d\mu_1\right)d\mu_2\right)\dots d\mu_{n-1}\right)d\mu_n$$. Alternatively, if we do not find an explicit formula for this calculation, we will show the general approach for this calculation, and prove the necessary conditions for its validity. 
\section{Notations}
\begin{itemize}
\item $\mathbb{Z}_p$, the ring of $p$-adic integers.
\item $\mathbb{Q}_p$, the fraction field of $\mathbb{Z}_p$.
\item $L_p$, a $\mathbb{Z}_p$-algebra over the ring of p-adic integers.
\item $\mathcal{L}_p$, a $\mathbb{Q}_p$-algebra, over the fraction field of $\mathbb{Z}_p$.
\item $G(L_p):=Aut_{\mathbb{Z}_p}(L_p)$, the group of $\mathbb{Z}_p$-automorphisms of $L_p$.
\item $G(\mathcal{L}_p):=Aut_{\mathbb{Q}_p}(\mathcal{L}_p)$, the group of $\mathbb{Q}_p$-automorphisms of $\mathcal{L}_p$.
\end{itemize}
\end{document}